{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the dataset name\n",
    "dataset_name = \"ktgiahieu/maccrobat2018_2020\"  # Replace with the desired dataset name\n",
    "\n",
    "# Download the dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Access the dataset\n",
    "MACCROBAT_train_dataset = dataset[\"train\"]\n",
    "# test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', '68', '-', 'year', '-', 'old', 'female', 'nonsmoker', ',', 'nondrinker', 'with', 'a', 'medical', 'history', 'of', 'hypertension', 'presented', 'with', 'new', '-', 'onset', 'painless', 'jaundice', 'and', 'pruritus', ',', 'a', 'three', '-', 'month', 'history', 'of', '9.9', 'kg', 'weight', 'loss', 'and', 'chronic', 'diarrhea', 'with', 'four', 'to', 'five', 'loose', 'bowel', 'movements', 'per', 'day', '.', '\\n', 'Medications', 'included', 'vitamin', 'D', ',', 'amlodipine', 'and', 'eprosartan', '.', '\\n', 'Physical', 'examination', 'was', 'normal', 'except', 'for', 'jaundice', 'and', 'muscle', 'wasting', '.', '\\n', 'Recent', 'colonoscopy', 'had', 'been', 'normal', '.', '\\n', 'Total', 'and', 'direct', 'bilirubin', 'levels', 'were', '6.84', 'mg', '/', 'dL', '(', '116.96', 'μmol', '/', 'L', ')', 'and', '9.18', 'mg', '/', 'dL', '(', '156.98', 'μmol', '/', 'L', ')', ',', 'respectively', '.', '\\n', 'Other', 'results', 'included', 'an', 'international', 'normalized', 'ratio', 'of', '1.0', ',', 'alanine', 'aminotransferase', 'level', '247', 'U', '/', 'L', '(', 'normal', '<33', 'U', '/', 'L', ')', ',', 'aspartate', 'aminotransferase', 'level', '139', 'U', '/', 'L', '(', 'normal', '<', '32', 'U', '/', 'L', ')', 'and', 'alkaline', 'phosphatase', 'level', '524', 'U', '/', 'L', '(', 'normal', '35', 'to', '104', 'U', '/', 'L', ')', '.', '\\n', 'Viral', 'hepatitis', 'serologies', ',', 'and', 'antimitochondrial', 'antibody', 'and', 'anti', '-', 'smooth', 'muscle', 'antibody', 'tests', 'were', 'negative', '.', '\\n', 'Her', 'alpha', '-', 'fetoprotein', 'level', 'was', '2.4', 'ng', '/', 'mL', '(', 'normal', '<', '5', 'ng', '/', 'mL', ')', ',', 'total', 'immunoglobulin', '(', 'Ig', ')', 'G', 'was', '1880', 'mg', '/', 'dL', '(', 'normal', '<', '640', 'mg', '/', 'dL', ')', ',', 'carbohydrate', 'antigen', '19', '-', '9', 'was', '856', 'U', '/', 'mL', '(', 'normal', '<33', 'U', '/', 'mL', ')', 'and', 'IgG4', 'was', '890', 'g', '/', 'L', '(', 'normal', '<3', 'g', '/', 'L', ')', '.', '\\n', 'Doppler', 'ultrasound', ',', 'magnetic', 'resonance', 'cholangiopancreatography', 'and', 'magnetic', 'resonance', 'imaging', 'of', 'the', 'liver', 'were', 'suspicious', 'for', 'a', 'subtly', 'enhancing', 'mass', '(', '2.8', 'cm', 'to', '4.2', 'cm', 'in', 'diameter', ')', 'in', 'the', 'region', 'of', 'the', 'hilum', 'and', 'porta', 'hepatis', ',', 'obstructing', 'both', 'the', 'right', 'and', 'left', 'hepatic', 'ducts', '.', '\\n', 'Endoscopic', 'retrograde', 'cholangiopancreatography', 'identified', 'strictures', 'in', 'the', 'central', 'portions', 'of', 'the', 'right', 'and', 'left', 'hepatic', 'duct', ',', 'which', 'was', 'concerning', 'for', 'cholangiocarcinoma', '(', 'Figure', '1', ')', '.', '\\n', 'Biliary', 'brushings', 'were', 'negative', 'for', 'malignancy', '.', '\\n', 'Esophagogastroduodenoscopy', 'was', 'normal', '.', '\\n', 'Biopsies', 'of', 'the', 'ampulla', 'of', 'Vater', 'revealed', 'chronic', 'active', 'duodenitis', '(', 'Figures', '2', 'and', '\\u200band3);3', ')', ';', 'an', 'ancillary', 'test', 'confirmed', 'the', 'diagnosis', '(', 'Figure', '4', ')', '.', '\\n', 'Treatment', 'with', 'corticosteroids', 'normalized', 'the', 'patient', '’s', 'biochemical', 'and', 'radiological', 'abnormalities', 'within', 'three', 'months', '.']\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate datasets created and saved in the 'output_datasets' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Your original dataset\n",
    "tokens = MACCROBAT_train_dataset['tokens'][0]\n",
    "ner_tags = MACCROBAT_train_dataset['tags'][0]\n",
    "\n",
    "# Step 1: Identify unique entity classes\n",
    "unique_entity_classes = set(tag.split('-')[-1] for tag in ner_tags if '-' in tag)\n",
    "\n",
    "# Step 2: Create separate datasets for each entity class\n",
    "for entity_class in unique_entity_classes:\n",
    "    entity_dataset_tokens = []\n",
    "    entity_dataset_ner_tags = []\n",
    "\n",
    "    for token, tag in zip(tokens, ner_tags):\n",
    "        if '-' in tag:\n",
    "            current_entity = tag.split('-')[-1]\n",
    "            if current_entity == entity_class:\n",
    "                # Retain only 'B', 'I', and 'O' tags\n",
    "                ner_tag = 'B' if tag.startswith('B') else 'I' if tag.startswith('I') else 'O'\n",
    "                entity_dataset_tokens.append(token)\n",
    "                entity_dataset_ner_tags.append(ner_tag)\n",
    "        else:\n",
    "            # If it's an 'O' tag, retain it\n",
    "            entity_dataset_tokens.append(token)\n",
    "            entity_dataset_ner_tags.append('O')\n",
    "\n",
    "    # Step 3: Save the new dataset to a separate file\n",
    "    output_folder = 'output_datasets'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_filename = os.path.join(output_folder, f'{entity_class}_dataset.txt')\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        for token, ner_tag in zip(entity_dataset_tokens, entity_dataset_ner_tags):\n",
    "            f.write(f'{token}\\t{ner_tag}\\n')\n",
    "\n",
    "print(\"Separate datasets created and saved in the 'output_datasets' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate datasets created and saved in the 'output_datasets' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Your dataset with multiple texts\n",
    "all_texts = MACCROBAT_train_dataset\n",
    "\n",
    "# Step 1: Identify unique entity classes\n",
    "unique_entity_classes = set()\n",
    "for text in all_texts:\n",
    "    ner_tags = text['tags']\n",
    "    unique_entity_classes.update(tag.split('-')[-1] for tag in ner_tags if '-' in tag)\n",
    "\n",
    "# Step 2 and 3: Create separate datasets and save them for each entity class\n",
    "for entity_class in unique_entity_classes:\n",
    "    entity_dataset_tokens = defaultdict(list)\n",
    "    entity_dataset_ner_tags = defaultdict(list)\n",
    "\n",
    "    for idx, text in enumerate(all_texts):\n",
    "        tokens = text['tokens']\n",
    "        ner_tags = text['tags']\n",
    "\n",
    "        for token, tag in zip(tokens, ner_tags):\n",
    "            if '-' in tag:\n",
    "                current_entity = tag.split('-')[-1]\n",
    "                if current_entity == entity_class:\n",
    "                    # Retain only 'B', 'I', and 'O' tags\n",
    "                    ner_tag = 'B' if tag.startswith('B') else 'I' if tag.startswith('I') else 'O'\n",
    "                    entity_dataset_tokens[idx].append(token)\n",
    "                    entity_dataset_ner_tags[idx].append(ner_tag)\n",
    "            else:\n",
    "                # If it's an 'O' tag, retain it\n",
    "                entity_dataset_tokens[idx].append(token)\n",
    "                entity_dataset_ner_tags[idx].append('O')\n",
    "\n",
    "    # Save the new dataset to a single file for each entity class\n",
    "    output_folder = 'output_datasets'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_filename = os.path.join(output_folder, f'{entity_class}_dataset.txt')\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        for idx in range(len(all_texts)):\n",
    "            for token, ner_tag in zip(entity_dataset_tokens[idx], entity_dataset_ner_tags[idx]):\n",
    "                f.write(f'{token}\\t{ner_tag}\\n')\n",
    "            # Add a newline between texts\n",
    "            f.write('\\n')\n",
    "\n",
    "print(\"Separate datasets created and saved in the 'output_datasets' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, test, and devel datasets created and saved in the 'output_datasets' folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Your dataset with multiple texts\n",
    "all_texts = MACCROBAT_train_dataset\n",
    "\n",
    "# Step 1: Identify unique entity classes\n",
    "unique_entity_classes = set()\n",
    "for text in all_texts:\n",
    "    ner_tags = text['tags']\n",
    "    unique_entity_classes.update(tag.split('-')[-1] for tag in ner_tags if '-' in tag)\n",
    "\n",
    "# Step 2 and 3: Create separate datasets and save them for each entity class\n",
    "for entity_class in unique_entity_classes:\n",
    "    entity_dataset_tokens = defaultdict(list)\n",
    "    entity_dataset_ner_tags = defaultdict(list)\n",
    "\n",
    "    for idx, text in enumerate(all_texts):\n",
    "        tokens = text['tokens']\n",
    "        ner_tags = text['tags']\n",
    "\n",
    "        for token, tag in zip(tokens, ner_tags):\n",
    "            if '-' in tag:\n",
    "                current_entity = tag.split('-')[-1]\n",
    "                if current_entity == entity_class:\n",
    "                    # Retain only 'B', 'I', and 'O' tags\n",
    "                    ner_tag = 'B' if tag.startswith('B') else 'I' if tag.startswith('I') else 'O'\n",
    "                    entity_dataset_tokens[idx].append(token)\n",
    "                    entity_dataset_ner_tags[idx].append(ner_tag)\n",
    "            else:\n",
    "                # If it's an 'O' tag, retain it\n",
    "                entity_dataset_tokens[idx].append(token)\n",
    "                entity_dataset_ner_tags[idx].append('O')\n",
    "\n",
    "    # Combine tokens and NER tags for each text\n",
    "    entity_datasets = []\n",
    "    for idx in range(len(all_texts)):\n",
    "        combined_data = list(zip(entity_dataset_tokens[idx], entity_dataset_ner_tags[idx]))\n",
    "        entity_datasets.append(combined_data)\n",
    "\n",
    "    # Split the dataset into train, test, and development sets\n",
    "    train_data, test_dev_data = train_test_split(entity_datasets, test_size=0.15, random_state=42)\n",
    "    test_data, devel_data = train_test_split(test_dev_data, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Save the datasets to separate files\n",
    "    output_folder = 'output_datasets'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for dataset, split_name in zip([train_data, test_data, devel_data], ['train', 'test', 'devel']):\n",
    "        subfolder_path = os.path.join(output_folder, f'{entity_class}')\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "        output_filename = os.path.join(subfolder_path, f'{split_name}.txt')\n",
    "\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            for combined_data in dataset:\n",
    "                for token, ner_tag in combined_data:\n",
    "                    f.write(f'{token} {ner_tag}\\n')  # Single space as the separator\n",
    "                # Add a newline between texts\n",
    "                f.write('\\n')\n",
    "\n",
    "print(\"Train, test, and devel datasets created and saved in the 'output_datasets' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the dataset name\n",
    "dataset_name = \"tner/bc5cdr\"  # Replace with the desired dataset name\n",
    "\n",
    "# Download the dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Access the dataset\n",
    "bc5cdr_train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset name\n",
    "dataset_name = \"jnlpba\"  # Replace with the desired dataset name\n",
    "\n",
    "# Download the dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Access the dataset\n",
    "jnlpba_train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset name\n",
    "dataset_name = \"bc2gm_corpus\"  # Replace with the desired dataset name\n",
    "\n",
    "# Download the dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Access the dataset\n",
    "bc2gm_corpus_train_dataset = dataset[\"train\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
