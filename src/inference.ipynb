{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'models/cache/'\n",
    "import medspacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import preprocessing_utils\n",
    "import utils\n",
    "import preprocessing\n",
    "from download import Downloader, download_study_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = joblib.Memory(\".\")\n",
    "\n",
    "def ParallelExecutor(use_bar=\"tqdm\", **joblib_args):\n",
    "    \"\"\"Utility for tqdm progress bar in joblib.Parallel\"\"\"\n",
    "    all_bar_funcs = {\n",
    "        \"tqdm\": lambda args: lambda x: tqdm(x, **args),\n",
    "        \"False\": lambda args: iter,\n",
    "        \"None\": lambda args: iter,\n",
    "    }\n",
    "    def aprun(bar=use_bar, **tq_args):\n",
    "        def tmp(op_iter):\n",
    "            if str(bar) in all_bar_funcs.keys():\n",
    "                bar_func = all_bar_funcs[str(bar)](tq_args)\n",
    "            else:\n",
    "                raise ValueError(\"Value %s not supported as bar type\" % bar)\n",
    "            \n",
    "            # Pass n_jobs from joblib_args\n",
    "            return joblib.Parallel(n_jobs=joblib_args.get(\"n_jobs\", 10))(bar_func(op_iter))\n",
    "\n",
    "        return tmp\n",
    "    return aprun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "# df = pd.read_csv(\"../data/clinicaltrials_parsed.csv\")\n",
    "# nct_ids = df[\"trials.nct_id\"].unique().tolist()\n",
    "\n",
    "folder_path = '../data/trials_xmls/'  # Replace this with the path to your folder\n",
    "file_names = []\n",
    "# List all files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if os.path.isfile(os.path.join(folder_path, file)):\n",
    "        file_name, file_extension = os.path.splitext(file)\n",
    "        file_names.append(file_name)\n",
    "        \n",
    "nct_ids = file_names\n",
    "\n",
    "def parallel_downloader(\n",
    "    n_jobs,\n",
    "    nct_ids,\n",
    "):\n",
    "    parallel_runner = ParallelExecutor(n_jobs=n_jobs)(total=len(nct_ids))\n",
    "    X = parallel_runner(\n",
    "        joblib.delayed(download_study_info)(\n",
    "        nct_id, \n",
    "        )\n",
    "        for nct_id in nct_ids\n",
    "    )     \n",
    "    updated_cts = np.vstack(X).flatten()\n",
    "    return updated_cts \n",
    "\n",
    "def parallel_preprocessing(\n",
    "    n_jobs,\n",
    "    nct_ids\n",
    "):\n",
    "    parallel_runner = ParallelExecutor(n_jobs=n_jobs)(total=len(nct_ids))\n",
    "    parallel_runner(\n",
    "        joblib.delayed(preprocessing.eic_text_preprocessing)(\n",
    "        [nct_id]\n",
    "        )\n",
    "        for nct_id in nct_ids\n",
    "    )       \n",
    "    \n",
    "# updated_cts = parallel_downloader(n_jobs=-1, nct_ids = nct_ids)\n",
    "\n",
    "parallel_preprocessing(\n",
    "    n_jobs=-1,\n",
    "    nct_ids = nct_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_utils.eic_text_preprocessing(_ids=[\"NCT05201183\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ids = [\"NCT00001586\"]\n",
    "for _, nid in enumerate(_ids):\n",
    "    print(nid)\n",
    "    eic_text = preprocessing_utils.extract_eligibility_criteria(nid)\n",
    "    # if eic_text is not None:\n",
    "    print(eic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_patterns = list(preprocessing_utils.load_regex_patterns(\"../data/regex_patterns.json\").values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Inclusion Criteria:\n",
    "\n",
    "          1. Documentation of Disease: Patients must be diagnosed with one of the following\n",
    "             conditions:\n",
    "\n",
    "               1. Acute Myeloid Leukemia (AML), with no history of extramedullary disease, who are\n",
    "                  not in complete remission, who have either primary refractory or relapsed\n",
    "                  disease, and who do not have more than one of the following adverse factors:\n",
    "\n",
    "                    -  Duration of first CR < 6 months (if previously in CR), based on the best\n",
    "                       overall clinical assessment of the disease course, not solely based on blood\n",
    "                       test or bone marrow biopsy results\n",
    "\n",
    "                    -  Poor risk karyotype including any of the following: complex karyotype with\n",
    "                       ≥3 clonal abnormalities, 5q-/-5, 7q-/-7, 11q23 abnormalities, inv(3q), 20q\n",
    "                       or 21q abnormalities, t (6;9), t (9;22), 17p abnormalities [or TP53\n",
    "                       mutations] or monosomal karyotype. Molecular typing (except for TP53\n",
    "                       mutation) will not be used for eligibility criteria determination.\n",
    "\n",
    "                    -  Circulating peripheral blood blasts at time of enrollment\n",
    "\n",
    "                    -  Karnofsky performance status <90%\n",
    "\n",
    "               2. Acute Lymphocytic Leukemia (ALL) who are not in complete remission, who have\n",
    "                  either primary refractory or relapsed disease, and who do not have more than one\n",
    "                  of the following adverse factors:\n",
    "\n",
    "                    -  Primary refractory or first relapse. Patients in second or subsequent\n",
    "                       relapse are excluded.\n",
    "\n",
    "                    -  Bone marrow blasts >25% within 30 days before the start of the conditioning\n",
    "                       regimen\n",
    "\n",
    "                    -  Age >40 years\n",
    "\n",
    "               3. Myelodysplasia with a Revised International Prognostic Score (IPSS-R) of greater\n",
    "                  than 4.5 (i.e., high- or very-high risk).\n",
    "\n",
    "               4. Chronic Myelogenous Leukemia (CML) in accelerated phase, defined by any of the\n",
    "                  following:\n",
    "\n",
    "                    -  10-19% blasts in peripheral blood white cells or bone marrow\n",
    "\n",
    "                    -  Peripheral blood basophils at least 20%\n",
    "\n",
    "                    -  Persistent thrombocytopenia (< 100 x 109/l) unrelated to therapy, or\n",
    "                       persistent thrombocytosis (>1000 x 109/l) unresponsive to therapy\n",
    "\n",
    "                    -  Increasing spleen size and increasing white blood cell (WBC) count\n",
    "                       unresponsive to therapy\n",
    "\n",
    "                    -  Cytogenetic evidence of clonal evolution (i.e., the appearance of an\n",
    "                       additional genetic abnormality that was not present in the initial specimen\n",
    "                       at the time of diagnosis of chronic phase)\n",
    "\n",
    "          2. The patient must be 18-65 years old at time of consent\n",
    "\n",
    "          3. Signed written informed consent: Patient must be capable of understanding the\n",
    "             investigational nature of this study, potential risks and benefits of the study, and\n",
    "             be able to provide a valid informed consent.\n",
    "\n",
    "          4. Availability of a consenting human leukocyte antigens (HLA)-matched donor\n",
    "\n",
    "          5. Karnofsky Performance Status 70% or higher\n",
    "\n",
    "          6. Required baseline laboratory values:\n",
    "\n",
    "               -  Estimated creatinine clearance ≥ 60 ml/min\n",
    "\n",
    "               -  Aspartate aminotransferase and alanine aminotransferase ≤ 2.5 x upper limit of\n",
    "                  normal value\n",
    "\n",
    "               -  Bilirubin ≤ 1.5 x upper limit of normal value (unless determined to be related to\n",
    "                  Gilbert's disease)\n",
    "\n",
    "          7. Required baseline cardiac function values:\n",
    "\n",
    "               -  Required baseline cardiac function of left ventricular ejection fraction (LVEF) >\n",
    "                  45 % corrected\n",
    "\n",
    "          8. Required baseline pulmonary function values:\n",
    "\n",
    "               -  Required baseline pulmonary function of lung diffusing capacity (DLCO) > 45 %\n",
    "                  predicted (corrected for hemoglobin))\n",
    "\n",
    "        Exclusion Criteria:\n",
    "\n",
    "          1. HIV seropositive patients\n",
    "\n",
    "          2. Pregnant or nursing females.\n",
    "\n",
    "          3. Prior radiation therapy\n",
    "\n",
    "          4. Patients who have had a prior autologous or allogeneic bone marrow or stem cell\n",
    "             transplantation\n",
    "\n",
    "          5. Gemtuzumab ozogamicin (trade name: Mylotarg) and/or inotuzumab ozogamicin (trade name:\n",
    "             Besponsa) use within 60 days before start of the conditioning regimen\n",
    "\n",
    "          6. Though this is NOT an exclusion criterion, we strongly recommend discontinuation of\n",
    "             any steroidal oral contraceptives at least 7 days before start of the conditioning\n",
    "             regimen. Use of therapeutic alternatives, including leuprolide should be considered to\n",
    "             reduce the risk of SOS/VOD. Of note, for patients already on steroidal oral\n",
    "             contraceptives for excessive menorrhagia, the switch to leuprolide should occur at\n",
    "             least 2 weeks before the start of the conditioning regimen\"\"\"\n",
    "             \n",
    "preprocessing_utils.split_to_sentences(text, regex_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy language models\n",
    "med_nlp = medspacy.load()\n",
    "tokenizer_biomedical = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\",  aggregation_strategy=\"first\")\n",
    "biomedical_pipeline = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", tokenizer=tokenizer_biomedical)\n",
    "mutations_tokenizer = AutoTokenizer.from_pretrained(\"Brizape/tmvar-PubMedBert-finetuned-24-02\")\n",
    "mutations_pipeline = pipeline(\"ner\", model=\"Brizape/tmvar-PubMedBert-finetuned-24-02\", tokenizer=mutations_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_entities = aux_pipeline(\"Engraftment including >95% myeloid cell donor chimerism and Absolute neutrophil count {ANC} > 1.0 x 109/L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_biomedical.tokenize(\"Engraftment including >95% myeloid cell donor chimerism and Absolute neutrophil count {ANC} > 1.0 x 109/L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from multiner_server import start_multiner_server\n",
    "# start_multiner_server()\n",
    "def query_plain(text, url=\"http://localhost:8888/plain\"):\n",
    "    return requests.post(url, json={'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Sleep for 3 seconds\n",
    "# time.sleep(4)\n",
    "ent_dict = query_plain(\"Currently receiving iron and vitamin B12 infusions for anemia with no resolution of fatigue\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict.content.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "run_path = \"../resources/BERN2/scripts/run_bern2.sh\"\n",
    "stop_path = \"../resources/BERN2/scripts/stop_bern2.sh\"\n",
    "working_directory = \"../resources/BERN2/scripts/\"\n",
    "os.chdir(working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "current_directory = os.getcwd()\n",
    "working_directory = \"../resources/BERN2/scripts/\"\n",
    "os.chdir(working_directory)\n",
    "run_path = \"run_bern2.sh\"\n",
    "stop_path = \"stop_bern2.sh\"\n",
    "print(\"Stopping any existing Multi-NER server instance.\")\n",
    "stop_process = subprocess.Popen([\"bash\", stop_path], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "stop_process.wait()\n",
    "print(\"Activating Mutli-NER Server... This can take approx. 1 minute\")\n",
    "try:\n",
    "    subprocess.Popen([\"bash\", run_path], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    timeout = 120  # Adjust this value as needed\n",
    "    # Define the server's URL that you want to check\n",
    "    server_url = \"http://localhost:8888\"  # Update with the actual URL\n",
    "    # Wait for the server to become available or reach the timeout\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            # Send a request to the server to check its availability\n",
    "            response = requests.get(server_url)\n",
    "            response.raise_for_status()  # Raises an exception for non-2xx status codes\n",
    "            break  # Server is available, exit the loop\n",
    "        except (requests.ConnectionError, requests.HTTPError) as e:\n",
    "            if time.time() - start_time >= timeout:\n",
    "                print(f\"Server did not become available within {timeout} seconds.\")\n",
    "                break  # Timeout reached\n",
    "            else:\n",
    "                # Wait for a short time before checking again\n",
    "                time.sleep(1)\n",
    "\n",
    "    # Continue with other tasks\n",
    "    print(\"Server is now available.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error executing the script: {e}\")\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtner_normalize_format(json_data):\n",
    "    spacy_format_entities = []\n",
    "    for annotation in json_data[\"annotations\"]:\n",
    "        start = annotation[\"span\"][\"begin\"]\n",
    "        end = annotation[\"span\"][\"end\"]\n",
    "        label = annotation[\"obj\"]\n",
    "        mention = annotation[\"mention\"]\n",
    "        score = annotation[\"prob\"]\n",
    "        normalized_id = annotation[\"id\"]\n",
    "        spacy_format_entities.append({\n",
    "            \"entity_group\": label,\n",
    "            \"text\": mention,\n",
    "            \"score\": score,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"normalized_id\": normalized_id\n",
    "        })\n",
    "    spacy_result = {\n",
    "        \"text\": json_data[\"text\"],\n",
    "        \"ents\": spacy_format_entities,\n",
    "    }\n",
    "\n",
    "    return spacy_result\n",
    "\n",
    "def post_process_entities(entity_list):\n",
    "    merged_entities = []\n",
    "    current_entity = None\n",
    "    for entity in entity_list:\n",
    "        current_entity = {\n",
    "            \"entity_group\": entity[\"entity_group\"],\n",
    "            \"score\": entity[\"score\"],\n",
    "            \"text\": entity[\"word\"].replace(\"##\", \" \"),\n",
    "            \"start\": entity[\"start\"],\n",
    "            \"end\": entity[\"end\"]\n",
    "        }\n",
    "        if (current_entity is not None) and entity[\"word\"].startswith(\"##\"):\n",
    "            current_entity[\"text\"] += entity[\"word\"].replace(\"##\", \"\")\n",
    "            current_entity[\"end\"] = entity[\"end\"]\n",
    "            current_entity[\"score\"] = max(current_entity[\"score\"], entity[\"score\"])\n",
    "            \n",
    "        else:\n",
    "            merged_entities.append(current_entity)\n",
    "            current_entity = None\n",
    "            \n",
    "    return merged_entities\n",
    "\n",
    "def merge_lists_with_priority_to_first(list1, list2):\n",
    "    merged_list = list1.copy()  # Create a copy of list1 to preserve its contents\n",
    "    \n",
    "    for dict2 in list2:\n",
    "        overlap = False\n",
    "        for dict1 in list1:\n",
    "            if (dict1['start'] <= dict2['end'] and dict2['start'] <= dict1['start']) or (dict2['start'] <= dict1['end'] and dict1['start'] <= dict2['start']):\n",
    "                overlap = True\n",
    "                break\n",
    "        \n",
    "        if not overlap:\n",
    "            merged_list.append(dict2)\n",
    "    \n",
    "    return merged_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/preprocessed_data/NCT05786924_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"For participants in the NSCLC Cohort: Known tumor programmed death-ligand 1 {PD-L1} expression status as determined by an immunohistochemistry assay during participation in other clinical studies {e.g., participants whose PD-L1 expression status was determined during screening for entry into a study with anti-programmed death 1 or anti-PD-L1 antibodies but were not eligible are excluded}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list = []\n",
    "for _,row in df.iterrows():\n",
    "    sentences = row[\"sentence\"].split(\".\")\n",
    "    for sent in sentences:\n",
    "        sent_dict = {}\n",
    "        sent_dict[\"sentence\"] = sent\n",
    "        main_entities = mtner_normalize_format(query_plain(sent))[\"ents\"]\n",
    "        variants_entities = mutations_pipeline(sent, aggregation_strategy=\"simple\")\n",
    "        combined_entities = merge_lists_with_priority_to_first(variants_entities, main_entities)\n",
    "        aux_entities = biomedical_pipeline(sent, aggregation_strategy=\"simple\")\n",
    "        \n",
    "        aux_entities = post_process_entities(get_dictionaries_with_values(aux_entities, \"entity_group\", [\"Age\", \"Sex\", \"Sign_symptom\", \"Biological_structure\", \"Date\", \n",
    "                                                                                        \"Duration\", \"Frequency\", \"Severity\", \"Lab_value\", \"Diagnostic_procedure\", \n",
    "                                                                                        \"Therapeutic_procedure\", \"Personal_background\", \"Clinical_event\", \"Outcome\"]))\n",
    "        combined_entities  = merge_lists_with_priority_to_first(combined_entities,aux_entities)\n",
    "        # Convert the selected_entries dictionary back to a list\n",
    "        sent_dict[\"annotations\"] = combined_entities\n",
    "        if len(sent_dict[\"annotations\"]) > 0:\n",
    "            ent_list.append(sent_dict)\n",
    "    # print(row[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_data/NCT05786924_preprocessed.csv\")\n",
    "all_dict = []\n",
    "for idx, row in df.iterrows():\n",
    "    sent_dict = {}\n",
    "    sent_dict[\"index\"]= idx + 1\n",
    "    doc = nlp(row[\"sentence\"])\n",
    "    text = \" \".join(doc._.bow)\n",
    "    sent_dict[\"sentence\"] = text\n",
    "    bern_entities= convert_to_spacy_format(query_plain(text))[\"ents\"]\n",
    "    mutation_entities = mutations_pipeline(text, aggregation_strategy=\"simple\")\n",
    "    combined_entities = merge_lists_with_priority(mutation_entities, bern_entities)\n",
    "    aux_entities = biomedical_pipeline(text, aggregation_strategy=\"simple\")\n",
    "    \n",
    "    aux_entities = post_process_entities(get_dictionaries_with_values(aux_entities, \"entity_group\", [\"Age\", \"Sex\", \"Sign_symptom\", \"Biological_structure\", \"Date\", \n",
    "                                                                                    \"Duration\", \"Frequency\", \"Severity\", \"Lab_value\", \"Diagnostic_procedure\", \n",
    "                                                                                    \"Therapeutic_procedure\", \"Personal_background\", \"Clinical_event\", \"Outcome\"]))\n",
    "    combined_entities  = merge_lists_with_priority(combined_entities,aux_entities)\n",
    "    # Convert the selected_entries dictionary back to a list\n",
    "    sent_dict[\"annotations\"] = combined_entities\n",
    "    if len(sent_dict[\"annotations\"]) > 0:\n",
    "        all_dict.append(sent_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Recurrent NSCLC with BRAF Class II alterations KRAS mutations other than TP53RK and G12C {ie, G12D, G12V} mutations {with Sponsor approval for KRAS mutations} without small cell lung cancer transformation with progressive disease confirmed by radiographic assessment.\".lower())\n",
    "text = \" \".join(doc._.bow)\n",
    "mutation_entities = mutations_pipeline(text, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_nlp = medspacy.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_nlp = medspacy.load()\n",
    "med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "med_nlp.disable_pipe('medspacy_pyrush')\n",
    "# med_nlp.add_pipe('sentencizer')\n",
    "print(med_nlp.pipe_names)\n",
    "@Language.component(\"gene-ner\")\n",
    "def gene_ner(doc):\n",
    "    spacy_entities = [(entity['entity_group'], entity['start'], entity['end']) for entity in entities_resolved]\n",
    "    for entity, start, end in spacy_entities:\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        # Find the corresponding tokens for the start and end positions\n",
    "        for token in doc:\n",
    "            if token.idx <= start < token.idx + len(token.text) and start_token is None:\n",
    "                start_token = token\n",
    "            if token.idx <= end <= token.idx + len(token.text) and end_token is None:\n",
    "                end_token = token\n",
    "\n",
    "        # Check if the start or end positions fall outside the tokenization\n",
    "        if start_token is None or end_token is None:\n",
    "            continue\n",
    "\n",
    "        span = spacy.tokens.Span(doc, start=start_token.i, end=end_token.i + 1, label=entity)\n",
    "        doc.ents = list(doc.ents) + [span]    \n",
    "    return doc\n",
    "\n",
    "\n",
    "med_nlp.add_pipe(\"gene-ner\", before='medspacy_context') \n",
    "\n",
    "@Language.component(\"biomed-ner\")\n",
    "def biomedical_ner(doc):\n",
    "    sp_entities = [(entity['entity_group'], entity['start'], entity['end']) for entity in entities_biomedical]\n",
    "    for entity, start, end in sp_entities:\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        # Find the corresponding tokens for the start and end positions\n",
    "        for token in doc:\n",
    "            if token.idx <= start < token.idx + len(token.text) and start_token is None:\n",
    "                start_token = token\n",
    "            if token.idx <= end <= token.idx + len(token.text) and end_token is None:\n",
    "                end_token = token\n",
    "\n",
    "        # Check if the start or end positions fall outside the tokenization\n",
    "        if start_token is None or end_token is None:\n",
    "            continue\n",
    "\n",
    "        span = spacy.tokens.Span(doc, start=start_token.i, end=end_token.i + 1, label=entity)\n",
    "        doc.ents = list(doc.ents) + [span]    \n",
    "    return doc\n",
    "\n",
    "# med_nlp.add_pipe(\"biomed-ner\", before='medspacy_context') \n",
    "\n",
    "@Language.component(\"aberrations-ner\")\n",
    "def regex_pattern_matcher_for_aberrations(doc):\n",
    "    df_regex = pd.read_csv(\"../data/regex_variants.tsv\", sep=\"\\t\", header=None)\n",
    "    df_regex = df_regex.rename(columns={1 : \"label\", 2:\"regex_pattern\"}).drop(columns=[0])\n",
    "    dict_regex = df_regex.set_index('label')['regex_pattern'].to_dict()\n",
    "    original_ents = list(doc.ents)\n",
    "    # Compile the regex patterns\n",
    "    compiled_patterns = {\n",
    "        label: re.compile(pattern)\n",
    "        for label, pattern in dict_regex.items()\n",
    "    }\n",
    "\n",
    "    mwt_ents = []\n",
    "    for label, pattern in compiled_patterns.items():\n",
    "        for match in re.finditer(pattern, doc.text):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end)\n",
    "            if span is not None:\n",
    "                mwt_ents.append((label, span.start, span.end, span.text))\n",
    "                \n",
    "    for ent in mwt_ents:\n",
    "        label, start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=label)\n",
    "        original_ents.append(per_ent)\n",
    "\n",
    "    doc.ents = filter_spans(original_ents)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "med_nlp.add_pipe(\"aberrations-ner\", before='medspacy_context') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list =[] \n",
    "for entity in doc.ents:\n",
    "    ent_list.append({\"entity_group\" : entity.label_, \"text\" : entity.text, \"start\": entity.start_char, \"end\": entity.end_char, \"is_negated\" : \"yes\" if entity._.is_negated else \"no\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aberration_recognizer(text):\n",
    "    med_nlp = medspacy.load()\n",
    "    med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "    @Language.component(\"aberrations-ner\")\n",
    "    def regex_pattern_matcher_for_aberrations(doc):\n",
    "        df_regex = pd.read_csv(\"../data/regex_variants.tsv\", sep=\"\\t\", header=None)\n",
    "        df_regex = df_regex.rename(columns={1 : \"label\", 2:\"regex_pattern\"}).drop(columns=[0])\n",
    "        dict_regex = df_regex.set_index('label')['regex_pattern'].to_dict()\n",
    "        original_ents = list(doc.ents)\n",
    "        # Compile the regex patterns\n",
    "        compiled_patterns = {\n",
    "            label: re.compile(pattern)\n",
    "            for label, pattern in dict_regex.items()\n",
    "        }\n",
    "        mwt_ents = []\n",
    "        for label, pattern in compiled_patterns.items():\n",
    "            for match in re.finditer(pattern, doc.text):\n",
    "                start, end = match.span()\n",
    "                span = doc.char_span(start, end)\n",
    "                if span is not None:\n",
    "                    mwt_ents.append((label, span.start, span.end, span.text))\n",
    "                    \n",
    "        for ent in mwt_ents:\n",
    "            label, start, end, name = ent\n",
    "            per_ent = Span(doc, start, end, label=label)\n",
    "            original_ents.append(per_ent)\n",
    "\n",
    "        doc.ents = filter_spans(original_ents)\n",
    "        \n",
    "        return doc\n",
    "    med_nlp.add_pipe(\"aberrations-ner\", before='medspacy_context')\n",
    "    doc = med_nlp(text)\n",
    "    return doc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = aberration_recognizer(\"Cohort 1: Recurrent NSCLC with BRAF Class II alterations or KRAS mutations other than G12C {ie, G12D, G12V} mutations {with Sponsor approval for KRAS mutations} without small cell lung cancer transformation with progressive disease confirmed by radiographic assessment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to modify options for displacy NER visualization\n",
    "def get_entity_options():\n",
    "    entities = [\n",
    "        \"Disease_disorder\", \"CHEMICAL\", \"Age\", \"GENETIC\", \"Duration\", \"Date\", \"Sex\",\n",
    "        \"Diagnostic_procedure\", \"Lab_value\", \"Protein\", \"DNA\", \"cell_type\",\n",
    "        \"Sign_symptom\", \"expression\", \"mutation\", \"NEG_ENTITY\",\n",
    "    ]\n",
    "    colors = {\n",
    "        \"Disease_disorder\": \"linear-gradient(180deg, #66ffcc, #abf763)\",\n",
    "        \"CHEMICAL\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "        \"Age\": \"linear-gradient(180deg, #ff9a8f, #ffb55e)\",\n",
    "        \"GENETIC\": \"linear-gradient(90deg, #9cd1fc, #9cfcf6)\",\n",
    "        \"Duration\": \"linear-gradient(180deg, #fe8ce6, #fe8cd9)\",\n",
    "        \"Date\": \"linear-gradient(90deg, #fca79c, #fcc59c)\",\n",
    "        \"Sex\": \"linear-gradient(180deg, #9cfdfe, #9c9dfc)\",\n",
    "        \"Diagnostic_procedure\": \"linear-gradient(90deg, #fcb69c, #fcec9c)\",\n",
    "        \"Lab_value\": \"linear-gradient(180deg, #9cfc9c, #e3fc9c)\",\n",
    "        \"Protein\": \"linear-gradient(90deg, #fc9cb0, #fc9cbe)\",\n",
    "        \"DNA\": \"linear-gradient(180deg, #9c9cfc, #a39cfc)\",\n",
    "        \"cell_type\": \"linear-gradient(90deg, #9ccdfc, #9cc3fc)\",\n",
    "        \"Sign_symptom\": \"linear-gradient(180deg, #9cfc9e, #d4fc9c)\",\n",
    "        \"expression\": \"linear-gradient(90deg, #9cfc9e, #fc9c9c)\",\n",
    "        \"mutation\": \"linear-gradient(180deg, #ffc766, #fc9c9c)\",\n",
    "        \"NEG_ENTITY\": \"linear-gradient(180deg, #ffc766, #fc9c9c)\"\n",
    "    }\n",
    "    options = {\"ents\": entities, \"colors\": colors}\n",
    "    return options\n",
    "\n",
    "options = get_entity_options()#visualizing identified Named Entities in clinical input text \n",
    "displacy.render(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_handling(doc):\n",
    "    results = []\n",
    "    for e in doc.ents:\n",
    "        rs = str(e._.is_negated)\n",
    "        if rs == \"True\": \n",
    "            results.append(e.text)\n",
    "    return results #list of negative concepts from clinical note identified by negspacy\n",
    "\n",
    "results0 = negation_handling(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to identify span objects of matched megative phrases from clinical note\n",
    "def match(nlp,terms,label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        matcher.add(label, None, *patterns)\n",
    "        return matcher\n",
    "    \n",
    "#replacing the labels for identified negative entities    \n",
    "def overwrite_ent_lbl(matcher, doc):\n",
    "    matches = matcher(doc)\n",
    "    seen_tokens = set()\n",
    "    new_entities = []\n",
    "    entities = doc.ents\n",
    "    for match_id, start, end in matches:\n",
    "        if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "            new_entities.append(Span(doc, start, end, label=match_id))\n",
    "            entities = [\n",
    "                e for e in entities if not (e.start < end and e.end > start)\n",
    "            ]\n",
    "            seen_tokens.update(range(start, end))\n",
    "        doc.ents = tuple(entities) + tuple(new_entities)\n",
    "    return doc\n",
    "\n",
    "matcher = match(med_nlp, results0, \"NEG_ENTITY\")\n",
    "#doc0: new doc object with added \"NEG_ENTITY label\"\n",
    "doc0 = overwrite_ent_lbl(matcher, doc) #visualizing identified Named Entities in clinical input text \n",
    "displacy.render(doc0, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add custom negation terms to the existing model\n",
    "from negspacy.termsets import termset\n",
    "ts = termset(\"en_clinical\")\n",
    "ts.add_patterns({\n",
    "            \"preceding_negations\": [\"deny\", \"refuse\", \"neither\", \"nor\", \"do not have\"],\n",
    "            \"following_negations\": [\"absence of\", \"deny\", \"decline\"],\n",
    "        })\n",
    "def neg_model2(nlp_model):\n",
    "    nlp = spacy.load(nlp_model, disable = ['parser'])\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    nlp.add_pipe(\"negex\")\n",
    "    return nlp  #updated list of all the negative concepts from clinical note identified by negspacy\n",
    "results1 = negation_handling(\"en_ner_bc5cdr_md\", lem_clinical_note, neg_model2)\n",
    "matcher = match(nlp1, results1, \"NEG_ENTITY\")\n",
    "#doc1: new doc object with added custom concepts for \"NEG_ENTITY label\"\n",
    "doc1 = overwrite_ent_lbl(matcher, doc) #visualizing identified Named Entities in clinical input text \n",
    "displacy.render(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "import pandas as pd\n",
    "from utils import tokenize\n",
    "\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "# Text to search for matches\n",
    "text = \"A Randomized Phase II Trial of a Mutated gp100 Melanoma Peptide g209-217210M With Hight Dose Interleukin-2 IL-2 in HLA-A2.1+Patients With Metastatic Melanoma\"\n",
    "\n",
    "@Language.component(\"aberrations-ner\")\n",
    "def regex_pattern_matcher_for_aberrations(doc):\n",
    "    df_regex = pd.read_csv(\"../data/regex_variants.tsv\", sep=\"\\t\", header=None)\n",
    "    df_regex = df_regex.rename(columns={1 : \"label\", 2:\"regex_pattern\"}).drop(columns=[0])\n",
    "    dict_regex = df_regex.set_index('label')['regex_pattern'].to_dict()\n",
    "    original_ents = list(doc.ents)\n",
    "    # Compile the regex patterns\n",
    "    compiled_patterns = {\n",
    "        label: re.compile(pattern)\n",
    "        for label, pattern in dict_regex.items()\n",
    "    }\n",
    "\n",
    "    mwt_ents = []\n",
    "    for label, pattern in compiled_patterns.items():\n",
    "        for match in re.finditer(pattern, doc.text):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end)\n",
    "            if span is not None:\n",
    "                mwt_ents.append((label, span.start, span.end, span.text))\n",
    "                \n",
    "    for ent in mwt_ents:\n",
    "        label, start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=label)\n",
    "        original_ents.append(per_ent)\n",
    "\n",
    "    doc.ents = filter_spans(original_ents)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"aberrations-ner\", last=True) \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Replace 'input_file.bin' and 'output_file.txt' with your file names\n",
    "\n",
    "# Read binary data from the .bin file\n",
    "with open('wikipedia-pubmed-and-PMC-w2v.bin', 'rb') as binary_file:\n",
    "    binary_data = binary_file.read()\n",
    "\n",
    "# Encode binary data to Base64\n",
    "base64_encoded_data = base64.b64encode(binary_data)\n",
    "\n",
    "# Convert bytes to a string and write to a .txt file\n",
    "with open('output_vectors.txt', 'w') as text_file:\n",
    "    text_file.write(base64_encoded_data.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the byte position from where you want to start printing\n",
    "# Number of bytes you want to print\n",
    "num_bytes_to_print = 100\n",
    "\n",
    "# Read binary data from the .bin file\n",
    "with open('wikipedia-pubmed-and-PMC-w2v.bin', 'rb') as binary_file:\n",
    "    binary_data = binary_file.read()\n",
    "\n",
    "# Slice the binary data to get only the first N bytes\n",
    "sliced_data = binary_data[6000000:6000100]\n",
    "print(sliced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Replace 'path_to_word2vec_model.bin' and 'output_file.txt' with appropriate values\n",
    "model = KeyedVectors.load_word2vec_format('wikipedia-pubmed-and-PMC-w2v.bin', binary=True)\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open('output_file.txt', 'w') as output_file:\n",
    "    # Iterate over each word in the vocabulary\n",
    "    for idx, word in enumerate(model.index_to_key):\n",
    "        print(idx)\n",
    "        # Get the word vector for the current word\n",
    "        word_vector = model.get_vector(word)\n",
    "\n",
    "        # Convert the word vector to a comma-separated string\n",
    "        vector_str = ','.join(str(val) for val in word_vector)\n",
    "\n",
    "        # Write the word and its vector to the file\n",
    "        output_file.write(f\"{word} {vector_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "# Replace 'path_to_word2vec_model.bin' and 'spacy_word2vec_model' with appropriate values\n",
    "gensim_model = KeyedVectors.load_word2vec_format('wikipedia-pubmed-and-PMC-w2v.bin', binary=True)\n",
    "\n",
    "# Create a new spaCy Language object with a blank Vocab\n",
    "nlp = spacy.blank(\"en\")\n",
    "vocab = Vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy word vectors from Gensim model to spaCy Vocab\n",
    "for word in gensim_model.index_to_key:\n",
    "    print(word)\n",
    "    vocab.set_vector(word, gensim_model[word])\n",
    "\n",
    "# Set the spaCy Vocab for the spaCy Language object\n",
    "nlp.vocab = vocab\n",
    "\n",
    "# # Save the spaCy model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"spacy_word2vec_biomed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "MESH_RDF_URL = \"https://id.nlm.nih.gov/mesh/sparql\"\n",
    "MESH_GRAPH = rdflib.Graph()\n",
    "\n",
    "def fetch_mesh_data():\n",
    "    MESH_GRAPH.parse(MESH_RDF_URL, format=\"xml\")\n",
    "\n",
    "# Fetch MeSH data\n",
    "fetch_mesh_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "py_text = \"Cohort 1: Recurrent NSCLC with BRAF Class II alterations or KRAS mutations other than G12C {ie, G12D, G12V} mutations {with Sponsor approval for KRAS mutations} without small cell lung cancer transformation with progressive disease confirmed by radiographic assessment.\"\n",
    "py_nlp = spacy.load(\"en_core_web_sm\")\n",
    "py_doc = py_nlp(py_text)\n",
    "displacy.render(py_doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "target_directory = \"/home/mabdallah/BERN2/multi_ner/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "# Specify the path to your .sh script and the directory you want to change to\n",
    "current_directory = os.getcwd()\n",
    "run_path = \"/home/mabdallah/BERN2/scripts/run_bern2.sh\"\n",
    "stop_path = \"/home/mabdallah/BERN2/scripts/stop_bern2.sh\"\n",
    "working_directory = \"/home/mabdallah/BERN2/scripts/\"\n",
    "os.chdir(working_directory)\n",
    "print(\"Stopping existing Bio-NER server instance.\")\n",
    "stop_process = subprocess.Popen([\"bash\", stop_path], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "stop_process.wait()\n",
    "print(\"Activating Bio-NER Server. This can take between 30 seconds and 1 minute.\")\n",
    "try:\n",
    "    subprocess.Popen([\"bash\", run_path], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    timeout = 60  # Adjust this value as needed\n",
    "    # Define the server's URL that you want to check\n",
    "    server_url = \"http://localhost:8888\"  # Update with the actual URL\n",
    "    # Wait for the server to become available or reach the timeout\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            # Send a request to the server to check its availability\n",
    "            response = requests.get(server_url)\n",
    "            response.raise_for_status()  # Raises an exception for non-2xx status codes\n",
    "            break  # Server is available, exit the loop\n",
    "        except (requests.ConnectionError, requests.HTTPError) as e:\n",
    "            if time.time() - start_time >= timeout:\n",
    "                print(f\"Server did not become available within {timeout} seconds.\")\n",
    "                break  # Timeout reached\n",
    "            else:\n",
    "                # Wait for a short time before checking again\n",
    "                time.sleep(1)\n",
    "\n",
    "    # Continue with other tasks\n",
    "    print(\"Server is now available.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error executing the script: {e}\")\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlap(tagged_docs, tmvar_docs):\n",
    "        \"\"\"\n",
    "        Step 1: check CUI and logit probability for same mention\n",
    "        Step 2: check overlap with mutation and tags with the highest probability\n",
    "        \"\"\"\n",
    "\n",
    "        # [Step 1] compare CUI and probability for same mention\n",
    "        span2mentions = {}\n",
    "        for entity_type, entity_dict in tagged_docs[0]['entities'].items():\n",
    "            # check CUI and probability\n",
    "            for mention_idx, mention_dict in enumerate(entity_dict):\n",
    "                start = mention_dict['start']\n",
    "                end = mention_dict['end']\n",
    "                if \"%d-%d\" % (start, end) not in span2mentions:\n",
    "                    span2mentions[\"%d-%d\" % (start, end)] = []\n",
    "                \n",
    "                span2mentions[\"%d-%d\"%(start, end)].append({\"type\":entity_type,\n",
    "                                                            \"CUI\": mention_dict['id'],\n",
    "                                                            \"check_CUI\": 1 if mention_dict['id'] != 'CUI-less' else 0,\n",
    "                                                            \"prob\": tagged_docs[0]['prob'][entity_type][mention_idx][1],\n",
    "                                                            \"is_neural_normalized\":mention_dict['is_neural_normalized']})\n",
    "        \n",
    "        for span in span2mentions.keys():\n",
    "            # sort elements with CUI\n",
    "            span2mentions[span] = sorted(span2mentions[span], key=lambda x:(x['check_CUI'], x['prob']), reverse=True)\n",
    "\n",
    "        for entity_type, entity_dict in tagged_docs[0]['entities'].items():\n",
    "            update_list = []\n",
    "            for mention_idx, mention_dict in enumerate(entity_dict):\n",
    "                start = mention_dict['start']\n",
    "                end = mention_dict['end']\n",
    "                \n",
    "                if span2mentions[\"%d-%d\"%(start, end)][0]['CUI'] == mention_dict['id'] and span2mentions[\"%d-%d\"%(start, end)][0]['type'] == entity_type:\n",
    "                    update_list.append(mention_dict)\n",
    "\n",
    "            tagged_docs[0]['entities'].update({entity_type:update_list})\n",
    "\n",
    "        # [Step 2] add mutation annotation\n",
    "        tagged_docs[0]['entities']['mutation'] = tmvar_docs[0]['entities']['mutation']\n",
    "        print(tmvar_docs)\n",
    "        return tagged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from downloader import Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Downloader(id_list=[\"NCT05786924\"], n_jobs=5).download_and_update_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootfile = \"../data/preprocessed_data/\"\n",
    "rootfile + \"/pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'path_to_directory' with the path of your directory\n",
    "directory_path = '/mnt/cbib/EOSC4Cancer/synthetic_data/'\n",
    "\n",
    "# Get the list of folders in the directory\n",
    "folder_list = [folder for folder in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, folder))]\n",
    "X = Preprocessor(id_list=folder_list, n_jobs=5).preprocess_patient_clinical_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_remove_overlaps(dictionary_list, preferred_groups):\n",
    "    # Create a dictionary to store non-overlapping entries\n",
    "    non_overlapping = {}\n",
    "\n",
    "    # Create a set from the preferred_groups for faster membership checking\n",
    "    preferred_set = set(preferred_groups)\n",
    "\n",
    "    # Iterate through the input list\n",
    "    for entry in dictionary_list:\n",
    "        text = entry['text']\n",
    "        group = entry['entity_group']\n",
    "\n",
    "        # Check if the text is already in the non_overlapping dictionary\n",
    "        if text in non_overlapping:\n",
    "            # Compare groups and keep the entry if it belongs to one of the preferred groups\n",
    "            if group in preferred_set:\n",
    "                non_overlapping[text] = entry\n",
    "        else:\n",
    "            non_overlapping[text] = entry\n",
    "\n",
    "    # Convert the non-overlapping dictionary back to a list\n",
    "    result_list = list(non_overlapping.values())\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_remove_overlaps(sentence[\"annotations\"], preferred_groups=[\"gene\", \"ProteinMutation\", \"DNAMutation\", \"SNP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_remove_overlaps(dictionary_list, preferred_groups):\n",
    "    # Create a dictionary to store non-overlapping entries\n",
    "    non_overlapping = {}\n",
    "\n",
    "    # Create a set from the preferred_groups for faster membership checking\n",
    "    preferred_set = set(preferred_groups)\n",
    "\n",
    "    # Iterate through the input list\n",
    "    for entry in dictionary_list:\n",
    "        text = entry['text']\n",
    "        group = entry['group']\n",
    "\n",
    "        # Check if the text is already in the non_overlapping dictionary\n",
    "        if text in non_overlapping:\n",
    "            # Compare groups and keep the entry if it belongs to one of the preferred groups\n",
    "            if group in preferred_set:\n",
    "                non_overlapping[text] = entry\n",
    "        else:\n",
    "            non_overlapping[text] = entry\n",
    "\n",
    "    # Convert the non-overlapping dictionary back to a list\n",
    "    result_list = list(non_overlapping.values())\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# Example usage:\n",
    "input_list = [\n",
    "    {'text': 'apple', 'group': 'A'},\n",
    "    {'text': 'banana', 'group': 'B'},\n",
    "    {'text': 'apple', 'group': 'B'},\n",
    "    {'text': 'banana', 'group': 'C'},\n",
    "    {'text': 'date', 'group': 'C'},\n",
    "]\n",
    "\n",
    "preferred_groups = ['A', 'B']\n",
    "\n",
    "result = find_and_remove_overlaps(input_list, preferred_groups)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_handling(sentence, entity):\n",
    "    med_nlp = medspacy.load()\n",
    "    med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "    # med_nlp.disable_pipe('medspacy_pyrush')\n",
    "    @Language.component(\"add_custom_entity\")\n",
    "    def add_cutom_entity(doc):\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        # Find the corresponding tokens for the start and end positions\n",
    "        for token in doc:\n",
    "            if token.idx <= entity[\"start\"] < token.idx + len(token.text) and start_token is None:\n",
    "                start_token = token\n",
    "            if token.idx <= entity[\"end\"] <= token.idx + len(token.text) and end_token is None:\n",
    "                end_token = token\n",
    "        doc.set_ents([Span(doc, start_token.i, end_token.i + 1, entity[\"entity_group\"])]) \n",
    "        return doc\n",
    "    med_nlp.add_pipe(\"add_custom_entity\", before='medspacy_context') \n",
    "    doc = med_nlp(sentence)\n",
    "    for e in doc.ents:\n",
    "        rs = str(e._.is_negated)\n",
    "        if rs == \"True\": \n",
    "            entity[\"is_negated\"] = \"yes\"\n",
    "        else:\n",
    "            entity[\"is_negated\"] = \"no\"\n",
    "    return  entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict = ent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict[[\"entity_group\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list[0][\"sentence\"] = \"No Histologically or cytologically confirmed recurrent/advanced metastatic solid tumors nor histiocytic neoplasms with documented BRAF or RAS {NRAS or KRAS} mutations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list[0][\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, World! (This) is an example sentence.\"\n",
    "\n",
    "# Remove punctuation using regex\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_recognition import EntityRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "folder_path = '../data/trials_xmls/'  # Replace this with the path to your folder\n",
    "file_names = []\n",
    "# List all files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if os.path.isfile(os.path.join(folder_path, file)):\n",
    "        file_name, file_extension = os.path.splitext(file)\n",
    "        file_names.append(file_name)\n",
    "\n",
    "reco = EntityRecognizer(n_jobs=5, id_list=file_names, data_source=\"clinical trials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mabdallah/TrialMatchAI/src/inference.ipynb Cell 74\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbaracuda/home/mabdallah/TrialMatchAI/src/inference.ipynb#Y133sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m entities \u001b[39m=\u001b[39m reco()\n",
      "File \u001b[0;32m/scratch/homedirs/mabdallah/TrialMatchAI/src/entity_recognition.py:465\u001b[0m, in \u001b[0;36mEntityRecognizer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m all_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_loader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_list)\n\u001b[1;32m    464\u001b[0m parallel_runner \u001b[39m=\u001b[39m ParallelExecutor(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_list))\n\u001b[0;32m--> 465\u001b[0m X \u001b[39m=\u001b[39m parallel_runner(\n\u001b[1;32m    466\u001b[0m     joblib\u001b[39m.\u001b[39;49mdelayed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecognize_entities)(\n\u001b[1;32m    467\u001b[0m     df, \n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    469\u001b[0m     \u001b[39mfor\u001b[39;49;00m df \u001b[39min\u001b[39;49;00m all_df\n\u001b[1;32m    470\u001b[0m )     \n\u001b[1;32m    471\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_source\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclinical trials\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m     pd\u001b[39m.\u001b[39mconcat(X)\u001b[39m.\u001b[39mto_csv(OUTPUT_FILEPATH_CT \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mentities_parsed.csv\u001b[39m\u001b[39m\"\u001b[39m, index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/scratch/homedirs/mabdallah/TrialMatchAI/src/entity_recognition.py:101\u001b[0m, in \u001b[0;36mParallelExecutor.<locals>.aprun.<locals>.tmp\u001b[0;34m(op_iter)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValue \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m not supported as bar type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m bar)\n\u001b[1;32m    100\u001b[0m \u001b[39m# Pass n_jobs from joblib_args\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m joblib\u001b[39m.\u001b[39;49mParallel(n_jobs\u001b[39m=\u001b[39;49mjoblib_args\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mn_jobs\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m10\u001b[39;49m))(bar_func(op_iter))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\", device=0) # pass device=0 if using gpu\n",
    "pipe(\"\"\"has not undergone a hysterectomy or bilateral oophorectomy\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexs = pd.read_csv(\"../data/regex_variants.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_dict = {0: \"pregnancy\", 1: \"pregnancy\", 2:\"\\b(?:pregnanc(?:y|ies|ial)?|expect(?:ing|ant)|matern(?:al|ity)|gravid|gestation(?:al)?|prenatal|antenatal|postpartum|lactat(?:e|ing|ion)|nurs(?:ing)?|breastfeed(?:ing|s)?)\\b\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexs = regexs.append(new_row_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "from spacy.tokens import Span\n",
    "import spacy\n",
    "entity = {'entity_group': 'Diagnostic_procedure', 'score': 0.99992955, 'text': 'eastern cooperative oncology group', 'start': 0, 'end': 34}\n",
    "sentence = \"Eastern Cooperative Oncology Group {ECOG} Performance Score {PS} of 0, 1 or 2\"\n",
    "\n",
    "def negation_handling(sentence, entity):\n",
    "        nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "        nlp = medspacy.load(nlp)\n",
    "        nlp.disable_pipe('medspacy_target_matcher')\n",
    "        nlp.disable_pipe('medspacy_pyrush')\n",
    "        print(entity[\"text\"])\n",
    "        @Language.component(\"add_custom_entity\")\n",
    "        def add_cutom_entity(doc):\n",
    "            print(doc)\n",
    "            start_char = doc.text.find(entity[\"text\"])\n",
    "            print(start_char)\n",
    "            end_char = start_char + len(entity[\"text\"]) - 1  # Subtract 1 to get the inclusive end position\n",
    "            print(end_char)\n",
    "            start_token = None\n",
    "            end_token = None\n",
    "            # Find the corresponding tokens for the start and end positions\n",
    "            for token in doc:\n",
    "                if token.idx <= start_char < token.idx + len(token.text) and start_token is None:\n",
    "                    start_token = token\n",
    "                if token.idx <= end_char <= token.idx + len(token.text) and end_token is None:\n",
    "                    end_token = token\n",
    "                if start_token is not None and end_token is not None:\n",
    "                    doc.set_ents([Span(doc, start_token.i, end_token.i + 1, entity[\"entity_group\"])]) \n",
    "            return doc\n",
    "        nlp.add_pipe(\"add_custom_entity\", before='medspacy_context') \n",
    "        doc = nlp(sentence.lower())\n",
    "        print(doc.ents)\n",
    "        for e in doc.ents:\n",
    "            rs = str(e._.is_negated)\n",
    "            # print(rs)\n",
    "            if rs == \"True\": \n",
    "                entity[\"is_negated\"] = \"yes\"\n",
    "            else:\n",
    "                entity[\"is_negated\"] = \"no\"\n",
    "        return  entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entss = negation_handling(sentence=sentence, entity=entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "nlp = medspacy.load(nlp)\n",
    "nlp.disable_pipe('medspacy_target_matcher')\n",
    "nlp.disable_pipe('medspacy_pyrush')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Platelets > 50 x 109/L with no platelet transfusions in the prior 7 days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is an example sentence with some text in it.\"\n",
    "substring = \"example sentence\"\n",
    "\n",
    "# Find the start and end positions of the substring\n",
    "start = sentence.find(substring)\n",
    "end = start + len(substring) - 1  # Subtract 1 to get the inclusive end position\n",
    "\n",
    "if start != -1:\n",
    "    print(f\"Start position: {start}\")\n",
    "    print(f\"End position: {end}\")\n",
    "else:\n",
    "    print(\"Substring not found in the sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "nlp = medspacy.load(nlp)\n",
    "nlp.disable_pipe('medspacy_target_matcher')\n",
    "nlp.disable_pipe('medspacy_pyrush')\n",
    "doc=nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start_char = doc.text.find(entity[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text for token in doc]\n",
    "combined_sentence = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def negation_handling(sentence, entity):\n",
    "    @Language.component(\"add_custom_entity\")\n",
    "    def add_custom_entity(doc):\n",
    "        threshold = 90\n",
    "        entity_text = entity[\"text\"].lower()\n",
    "        # Convert the document tokens to a list of token texts\n",
    "        token_texts = [token.text for token in doc]\n",
    "        start_indices = []\n",
    "\n",
    "        for i in range(len(token_texts) - len(entity_text.split()) + 1):\n",
    "            window = \" \".join(token_texts[i:i + len(entity_text.split())])\n",
    "            if fuzz.partial_ratio(entity_text, window) >= threshold:\n",
    "                start_indices.append(i)\n",
    "\n",
    "        if start_indices:\n",
    "            print(start_indices)\n",
    "            # You can choose the first matching window or handle multiple matches\n",
    "            start_index = start_indices[0]\n",
    "            start_token = doc[start_index]\n",
    "            print(start_token)\n",
    "            end_token = doc[start_index + len(entity_text.split()) - 1]\n",
    "            print(doc[start_token.i:end_token.i + 1])\n",
    "            doc.set_ents([Span(doc, start_token.i, end_token.i + 1, entity[\"entity_group\"])])\n",
    "        return doc\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "    nlp = medspacy.load(nlp)\n",
    "    nlp.disable_pipe('medspacy_target_matcher')\n",
    "    nlp.disable_pipe('medspacy_pyrush')\n",
    "    nlp.add_pipe(\"add_custom_entity\", before='medspacy_context') \n",
    "    doc = nlp(sentence.lower())\n",
    "    for e in doc.ents:\n",
    "        rs = str(e._.is_negated)\n",
    "        if rs == \"True\": \n",
    "            entity[\"is_negated\"] = \"yes\"\n",
    "        else:\n",
    "            entity[\"is_negated\"] = \"no\"\n",
    "    return  entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"willing and able to adhere to the study visit schedule and other protocol requirements.\"\n",
    "entity = {'entity_group': 'Lab_value', 'score': 0.9387666, 'text': 'willing', 'start': 0, 'end': 7}\n",
    "# negation_handling(sentence, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity[\"text\"].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your text\n",
    "text = \"engraftment including >95% myeloid cell donor chimerism and absolute neutrophil count {anc} > 1.0 x 109/l\"\n",
    "entity_text = \"> 1. 0 x 109 / l\"\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Character indices\n",
    "start_char = 92\n",
    "end_char = 105\n",
    "\n",
    "# Find the token indices corresponding to the character span\n",
    "token_indices = [i for i, token in enumerate(doc) if start_char <= token.idx < end_char]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Token indices corresponding to character span ({start_char}, {end_char}): {token_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import medspacy\n",
    "entity= {'entity_group': 'Lab_value', 'score': 0.8586178, 'text': '- 14. 2', 'start': 17, 'end': 22}\n",
    "sentence = \"06:30AM BLOOD WBC-14.2* RBC-3.82* Hgb-11.0*# Hct-33.5*\"\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "doc = nlp(sentence)\n",
    "nlp = medspacy.load(nlp)\n",
    "# doc = nlp(sentence)\n",
    "entity[\"text\"] = re.sub(r'([,.-])\\s+', r'\\1', entity[\"text\"]) \n",
    "# print(entity[\"text\"])\n",
    "entity_text = entity[\"text\"].lower()\n",
    "start_char = entity[\"start\"] \n",
    "end_char = entity[\"end\"] \n",
    "start_indices = [i for i, token in enumerate(doc) if (start_char <= token.idx <= end_char) or (entity_text in token.text and token.idx <= start_char and token.idx + len(token.text) <= end_char)]\n",
    "print(start_indices)\n",
    "if start_indices:\n",
    "# You can choose the first matching window or handle multiple matches\n",
    "    start_index = start_indices[0]\n",
    "    start_token = doc[start_index]\n",
    "    end_index = min(start_index + len(entity_text.split()) - 1, len(doc) - 1)\n",
    "    end_token = doc[end_index]\n",
    "    # print(doc[start_token.i:end_token.i + 1])\n",
    "    doc.set_ents([Span(doc, start_token.i, end_token.i + 1, entity[\"entity_group\"])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(doc):\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "start_indices = [i for i, token in enumerate(doc) if start_char <= token.idx < end_char]\n",
    "if start_indices:\n",
    "# You can choose the first matching window or handle multiple matches\n",
    "    start_index = start_indices[0]\n",
    "    start_token = doc[start_index]\n",
    "    end_index = min(start_index + len(entity_text.split()) - 1, len(doc) - 1)\n",
    "    end_token = doc[end_index]\n",
    "    # print(doc[start_token.i:end_token.i + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entity_text.split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [\n",
    "    {\n",
    "        \"entity_group\": \"Lab_value\",\n",
    "        \"score\": 0.9990455508232117,\n",
    "        \"word\": \"<\",\n",
    "        \"start\": 11,\n",
    "        \"end\": 12\n",
    "    },\n",
    "    {\n",
    "        \"entity_group\": \"Lab_value\",\n",
    "        \"score\": 0.9935429096221924,\n",
    "        \"word\": \"1. 5\",\n",
    "        \"start\": 13,\n",
    "        \"end\": 16\n",
    "    },\n",
    "    {\n",
    "        \"entity_group\": \"Lab_value\",\n",
    "        \"score\": 0.9999258518218994,\n",
    "        \"word\": \"normal\",\n",
    "        \"start\": 19,\n",
    "        \"end\": 25\n",
    "    },\n",
    "]\n",
    "\n",
    "def combine_entities(entities):\n",
    "    combined_entities = []\n",
    "    current_entity = entities[0]\n",
    "    for next_entity in entities[1:]:\n",
    "        if (\n",
    "            current_entity['entity_group'] == next_entity['entity_group']\n",
    "            and next_entity['start'] - current_entity['end'] <= 3\n",
    "        ):\n",
    "            current_entity['word'] += ' ' + next_entity['word']\n",
    "            current_entity['end'] = next_entity['end']\n",
    "        else:\n",
    "            combined_entities.append(current_entity)\n",
    "            current_entity = next_entity\n",
    "\n",
    "    combined_entities.append(current_entity)\n",
    "    return combined_entities\n",
    "\n",
    "combined = combine_entities(entities)\n",
    "print(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # You can choose different models from Hugging Face's repository\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Original sentence and its negation\n",
    "sentence = \"fantastic.\"\n",
    "negation_sentence = \"not fantastic.\"\n",
    "\n",
    "# Tokenize and get IDs for the sentences\n",
    "inputs_sentence = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs_negation = tokenizer(negation_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "with torch.no_grad():\n",
    "    outputs_sentence = model(**inputs_sentence)\n",
    "    outputs_negation = model(**inputs_negation)\n",
    "\n",
    "# Extract the embeddings (CLS token) from the last layer\n",
    "embedding_sentence = outputs_sentence.last_hidden_state[:, 0, :].numpy()\n",
    "embedding_negation = outputs_negation.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Compute cosine similarity between the embeddings\n",
    "similarity_score = cosine_similarity(embedding_sentence, embedding_negation)\n",
    "\n",
    "# Print the sentences and their similarity score\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Negation Sentence:\", negation_sentence)\n",
    "print(\"Cosine Similarity:\", similarity_score[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "database = client['your_database_name']\n",
    "collection = database['your_collection_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB (assuming it's running locally on default port)\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "database = client['test_database']  # Change 'test_database' to your database name\n",
    "collection = database['test_collection']  # Change 'test_collection' to your collection name\n",
    "\n",
    "# Insert some sample data into the collection\n",
    "data_to_insert = [\n",
    "    {\n",
    "        \"name\": \"John\",\n",
    "        \"age\": 30,\n",
    "        \"address\": {\n",
    "            \"city\": \"New York\",\n",
    "            \"zipcode\": \"10001\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Alice\",\n",
    "        \"age\": 25,\n",
    "        \"address\": {\n",
    "            \"city\": \"San Francisco\",\n",
    "            \"zipcode\": \"94107\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"age\": 35,\n",
    "        \"address\": {\n",
    "            \"city\": \"Los Angeles\",\n",
    "            \"zipcode\": \"90001\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Insert the data into the collection\n",
    "collection.insert_many(data_to_insert)\n",
    "\n",
    "# Perform a search/query\n",
    "# Find documents where the city in the address is \"New York\"\n",
    "result = collection.find({\"address.city\": \"New York\"})\n",
    "\n",
    "# Iterate through the results and print them\n",
    "for doc in result:\n",
    "    print(doc)\n",
    "\n",
    "# Close the connection to MongoDB\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "premise = \"'gene': KRAS, 'is_negated':no\"\n",
    "hypothesis = \"'gene': KRAS, 'is_negated':yes\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_space_between_numbers(text):\n",
    "    import re\n",
    "    # Use regular expression to find space between numbers and remove it\n",
    "    modified_text = re.sub(r'(\\d)[\\s,]+(\\d)(?!\\D)', r'\\1\\2', text)\n",
    "    return modified_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "texts_with_spaces = [\n",
    "   \"1- 2\"\n",
    "]\n",
    "\n",
    "texts_without_spaces = [re.sub(r'([,.-])\\s+', r'\\1', text) for text in texts_with_spaces]\n",
    "\n",
    "for text in texts_without_spaces:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_nct = pd.read_csv(\"../data/ner_clinical_trials/entities_parsed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_nct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\") # pass device=0 if using gpu\n",
    "pipe(\"\"\"For participants in the NSCLC Cohort: Known tumor programmed death-ligand 1 {PD-L1} expression status as determined by an immunohistochemistry assay during participation in other clinical studies {e.g., participants whose PD-L1 expression status was determined during screening for entry into a study with anti-programmed death 1 or anti-PD-L1 antibodies but were not eligible are excluded}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from spacy.tokens import Span\n",
    "\n",
    "def pregnancy_recognizer(self, text):\n",
    "    med_nlp = medspacy.load()\n",
    "    med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "    \n",
    "    # Updated regex pattern\n",
    "    regex_pattern = r\"(?i)\\b(?:pregn\\w+|matern\\w+|gestat\\w+|lactat\\w+|breastfeed\\w+|prenat\\w+|antenat\\w+|postpartum|childbear\\w+|parturient|conceiv\\w+|obstetr\\w+)\\b\"\n",
    "\n",
    "    @Language.component(\"pregnancy-ner\")\n",
    "    def regex_pattern_matcher_for_pregnancy(doc):\n",
    "        compiled_pattern = re.compile(regex_pattern)\n",
    "\n",
    "        original_ents = list(doc.ents)\n",
    "        mwt_ents = []\n",
    "\n",
    "        for match in re.finditer(compiled_pattern, doc.text):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end)\n",
    "            if span is not None:\n",
    "                mwt_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "        for ent in mwt_ents:\n",
    "            start, end, name = ent\n",
    "            per_ent = Span(doc, start, end, label=\"pregnancy\")  # Assigning the label \"pregnancy\"\n",
    "            original_ents.append(per_ent)\n",
    "\n",
    "        doc.ents = filter_spans(original_ents)\n",
    "\n",
    "        return doc\n",
    "\n",
    "    med_nlp.add_pipe(\"pregnancy-ner\", before='medspacy_context')\n",
    "    doc = med_nlp(text)\n",
    "    \n",
    "    ent_list =[] \n",
    "    for entity in doc.ents:\n",
    "        ent_list.append({\n",
    "            \"entity_group\": entity.label_,\n",
    "            \"text\": entity.text,\n",
    "            \"start\": entity.start_char,\n",
    "            \"end\": entity.end_char,\n",
    "            \"is_negated\": \"yes\" if entity._.is_negated else \"no\"\n",
    "        })\n",
    "    \n",
    "    return ent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_cancer_trials():\n",
    "    base_url = \"https://clinicaltrials.gov/api/query/full_studies\"\n",
    "    trials_list = []\n",
    "\n",
    "    start = 0\n",
    "    rows = 100  # Maximum rows allowed per request\n",
    "\n",
    "    while True:\n",
    "        search_params = {\n",
    "            \"expr\": \"(cancer) AND (Interventional) AND (mutation)\",\n",
    "            \"min_rnk\": start + 1,\n",
    "            \"max_rnk\": start + rows,\n",
    "            \"fmt\": \"json\",\n",
    "            \"fields\": \"NCTId\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=search_params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            trials_data = response.json()\n",
    "            if \"FullStudiesResponse\" in trials_data:\n",
    "                studies = trials_data[\"FullStudiesResponse\"][\"FullStudies\"]\n",
    "                if not studies:  # No more records\n",
    "                    break\n",
    "\n",
    "                for study in studies:\n",
    "                    trials_list.append(study[\"Study\"][\"ProtocolSection\"][\"IdentificationModule\"][\"NCTId\"])\n",
    "\n",
    "                start += rows\n",
    "            else:\n",
    "                print(\"No trials found matching the criteria.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to retrieve data. Status code:\", response.status_code)\n",
    "            break\n",
    "\n",
    "    return trials_list\n",
    "\n",
    "# Example usage:\n",
    "cancer_trials = get_cancer_trials()\n",
    "print(\"Clinical trial IDs related to cancer, intervention, and mutations:\")\n",
    "print(cancer_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cancer_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_cancer_trials(max_trials):\n",
    "    base_url = \"https://clinicaltrials.gov/api/query/full_studies\"\n",
    "    trials_set = set()\n",
    "    page_size = 100  # Number of trials per page\n",
    "    current_rank = 1\n",
    "    trials_fetched = 0\n",
    "\n",
    "    while trials_fetched < max_trials:\n",
    "        search_params = {\n",
    "            \"expr\": \"((cancer) OR (neoplasm)) AND ((interventional) OR (treatment)) AND ((mutation) OR (variant))\",\n",
    "            \"min_rnk\": current_rank,\n",
    "            \"max_rnk\": current_rank + page_size - 1,\n",
    "            \"fmt\": \"json\",\n",
    "            \"fields\": \"NCTId\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=search_params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            trials_data = response.json()\n",
    "            if \"FullStudiesResponse\" in trials_data:\n",
    "                studies = trials_data[\"FullStudiesResponse\"][\"FullStudies\"]\n",
    "                if not studies:\n",
    "                    break  # No more studies found, exit the loop\n",
    "                for study in studies:\n",
    "                    trials_set.add(study[\"Study\"][\"ProtocolSection\"][\"IdentificationModule\"][\"NCTId\"])\n",
    "                    trials_fetched += 1\n",
    "                    if trials_fetched == max_trials:\n",
    "                        break\n",
    "                current_rank += page_size\n",
    "            else:\n",
    "                print(\"No trials found matching the criteria.\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Failed to retrieve data. Status code:\", response.status_code)\n",
    "            break\n",
    "\n",
    "    return list(trials_set)  # Convert set to list for output\n",
    "\n",
    "# Example usage: Fetching a maximum of 500 trials\n",
    "max_trials_to_fetch = 1000\n",
    "cancer_trials = get_cancer_trials(max_trials_to_fetch)\n",
    "print(\"Clinical trial IDs related to cancer, intervention, and mutations (up to {} trials):\".format(max_trials_to_fetch))\n",
    "print(cancer_trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_plain(text, url=\"http://localhost:8888/plain\"):\n",
    "    return requests.post(url, json={'text': text}).json()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = \"Autophagy maintains tumour growth through circulating arginine.\"\n",
    "    print(query_plain(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiner_server import start_multiner_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_multiner_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
