{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'models/cache/'\n",
    "import medspacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import preprocessing\n",
    "import utils\n",
    "from utils import get_dictionaries_with_values, download_study_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = joblib.Memory(\".\")\n",
    "\n",
    "def ParallelExecutor(use_bar=\"tqdm\", **joblib_args):\n",
    "    \"\"\"Utility for tqdm progress bar in joblib.Parallel\"\"\"\n",
    "    all_bar_funcs = {\n",
    "        \"tqdm\": lambda args: lambda x: tqdm(x, **args),\n",
    "        \"False\": lambda args: iter,\n",
    "        \"None\": lambda args: iter,\n",
    "    }\n",
    "    def aprun(bar=use_bar, **tq_args):\n",
    "        def tmp(op_iter):\n",
    "            if str(bar) in all_bar_funcs.keys():\n",
    "                bar_func = all_bar_funcs[str(bar)](tq_args)\n",
    "            else:\n",
    "                raise ValueError(\"Value %s not supported as bar type\" % bar)\n",
    "            \n",
    "            # Pass n_jobs from joblib_args\n",
    "            return joblib.Parallel(n_jobs=joblib_args.get(\"n_jobs\", 10))(bar_func(op_iter))\n",
    "\n",
    "        return tmp\n",
    "    return aprun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df = pd.read_csv(\"../data/clinicaltrials_parsed.csv\")\n",
    "nct_ids = df[\"trials.nct_id\"].unique().tolist()\n",
    "nct_ids = [\"NCT05786924\"]\n",
    "\n",
    "def parallel_downloader(\n",
    "    n_jobs,\n",
    "    nct_ids,\n",
    "):\n",
    "    parallel_runner = ParallelExecutor(n_jobs=n_jobs)(total=len(nct_ids))\n",
    "    X = parallel_runner(\n",
    "        joblib.delayed(download_study_info)(\n",
    "        nct_id, \n",
    "        )\n",
    "        for nct_id in nct_ids\n",
    "    )     \n",
    "    updated_cts = np.vstack(X).flatten()\n",
    "    return updated_cts \n",
    "\n",
    "def parallel_preprocessing(\n",
    "    n_jobs,\n",
    "    nct_ids\n",
    "):\n",
    "    parallel_runner = ParallelExecutor(n_jobs=n_jobs)(total=len(nct_ids))\n",
    "    parallel_runner(\n",
    "        joblib.delayed(preprocessing.eic_text_preprocessing)(\n",
    "        [nct_id]\n",
    "        )\n",
    "        for nct_id in nct_ids\n",
    "    )       \n",
    "    \n",
    "updated_cts = parallel_downloader(n_jobs=-1, nct_ids = nct_ids)\n",
    "\n",
    "parallel_preprocessing(\n",
    "    n_jobs=-1,\n",
    "    nct_ids = nct_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy language models\n",
    "med_nlp = medspacy.load()\n",
    "tokenizer_biomedical = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "biomedical_pipeline = pipeline(\"ner\", model=\"d4data/biomedical-ner-all\", tokenizer=tokenizer_biomedical)\n",
    "mutations_tokenizer = AutoTokenizer.from_pretrained(\"Brizape/tmvar-PubMedBert-finetuned-24-02\")\n",
    "mutations_pipeline = pipeline(\"ner\", model=\"Brizape/tmvar-PubMedBert-finetuned-24-02\", tokenizer=mutations_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_plain(text, url=\"http://localhost:8888/plain\"):\n",
    "    return requests.post(url, json={'text': text}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Sleep for 3 seconds\n",
    "# time.sleep(4)\n",
    "ent_dict = query_plain(\"\"\"KRAS gene mutation\"\"\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtner_normalize_format(json_data):\n",
    "    spacy_format_entities = []\n",
    "    for annotation in json_data[\"annotations\"]:\n",
    "        start = annotation[\"span\"][\"begin\"]\n",
    "        end = annotation[\"span\"][\"end\"]\n",
    "        label = annotation[\"obj\"]\n",
    "        mention = annotation[\"mention\"]\n",
    "        score = annotation[\"prob\"]\n",
    "        normalized_id = annotation[\"id\"]\n",
    "        spacy_format_entities.append({\n",
    "            \"entity_group\": label,\n",
    "            \"text\": mention,\n",
    "            \"score\": score,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"normalized_id\": normalized_id\n",
    "        })\n",
    "    spacy_result = {\n",
    "        \"text\": json_data[\"text\"],\n",
    "        \"ents\": spacy_format_entities,\n",
    "    }\n",
    "\n",
    "    return spacy_result\n",
    "\n",
    "def post_process_entities(entity_list):\n",
    "    merged_entities = []\n",
    "    current_entity = None\n",
    "    for entity in entity_list:\n",
    "        current_entity = {\n",
    "            \"entity_group\": entity[\"entity_group\"],\n",
    "            \"score\": entity[\"score\"],\n",
    "            \"text\": entity[\"word\"].replace(\"##\", \" \"),\n",
    "            \"start\": entity[\"start\"],\n",
    "            \"end\": entity[\"end\"]\n",
    "        }\n",
    "        if (current_entity is not None) and entity[\"word\"].startswith(\"##\"):\n",
    "            current_entity[\"text\"] += entity[\"word\"].replace(\"##\", \"\")\n",
    "            current_entity[\"end\"] = entity[\"end\"]\n",
    "            current_entity[\"score\"] = max(current_entity[\"score\"], entity[\"score\"])\n",
    "            \n",
    "        else:\n",
    "            merged_entities.append(current_entity)\n",
    "            current_entity = None\n",
    "            \n",
    "    return merged_entities\n",
    "\n",
    "def merge_lists_with_priority_to_first(list1, list2):\n",
    "    merged_list = list1.copy()  # Create a copy of list1 to preserve its contents\n",
    "    \n",
    "    for dict2 in list2:\n",
    "        overlap = False\n",
    "        for dict1 in list1:\n",
    "            if (dict1['start'] <= dict2['end'] and dict2['start'] <= dict1['start']) or (dict2['start'] <= dict1['end'] and dict1['start'] <= dict2['start']):\n",
    "                overlap = True\n",
    "                break\n",
    "        \n",
    "        if not overlap:\n",
    "            merged_list.append(dict2)\n",
    "    \n",
    "    return merged_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/preprocessed_data/NCT05786924_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list = []\n",
    "for _,row in df.iterrows():\n",
    "    sentences = row[\"sentence\"].split(\".\")\n",
    "    for sent in sentences:\n",
    "        sent_dict = {}\n",
    "        sent_dict[\"sentence\"] = sent\n",
    "        main_entities = mtner_normalize_format(query_plain(sent))[\"ents\"]\n",
    "        variants_entities = mutations_pipeline(sent, aggregation_strategy=\"simple\")\n",
    "        combined_entities = merge_lists_with_priority_to_first(variants_entities, main_entities)\n",
    "        aux_entities = biomedical_pipeline(sent, aggregation_strategy=\"simple\")\n",
    "        \n",
    "        aux_entities = post_process_entities(get_dictionaries_with_values(aux_entities, \"entity_group\", [\"Age\", \"Sex\", \"Sign_symptom\", \"Biological_structure\", \"Date\", \n",
    "                                                                                        \"Duration\", \"Frequency\", \"Severity\", \"Lab_value\", \"Diagnostic_procedure\", \n",
    "                                                                                        \"Therapeutic_procedure\", \"Personal_background\", \"Clinical_event\", \"Outcome\"]))\n",
    "        combined_entities  = merge_lists_with_priority_to_first(combined_entities,aux_entities)\n",
    "        # Convert the selected_entries dictionary back to a list\n",
    "        sent_dict[\"annotations\"] = combined_entities\n",
    "        if len(sent_dict[\"annotations\"]) > 0:\n",
    "            ent_list.append(sent_dict)\n",
    "    # print(row[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_data/NCT05786924_preprocessed.csv\")\n",
    "all_dict = []\n",
    "for idx, row in df.iterrows():\n",
    "    sent_dict = {}\n",
    "    sent_dict[\"index\"]= idx + 1\n",
    "    doc = nlp(row[\"sentence\"])\n",
    "    text = \" \".join(doc._.bow)\n",
    "    sent_dict[\"sentence\"] = text\n",
    "    bern_entities= convert_to_spacy_format(query_plain(text))[\"ents\"]\n",
    "    mutation_entities = mutations_pipeline(text, aggregation_strategy=\"simple\")\n",
    "    combined_entities = merge_lists_with_priority(mutation_entities, bern_entities)\n",
    "    aux_entities = biomedical_pipeline(text, aggregation_strategy=\"simple\")\n",
    "    \n",
    "    aux_entities = post_process_entities(get_dictionaries_with_values(aux_entities, \"entity_group\", [\"Age\", \"Sex\", \"Sign_symptom\", \"Biological_structure\", \"Date\", \n",
    "                                                                                    \"Duration\", \"Frequency\", \"Severity\", \"Lab_value\", \"Diagnostic_procedure\", \n",
    "                                                                                    \"Therapeutic_procedure\", \"Personal_background\", \"Clinical_event\", \"Outcome\"]))\n",
    "    combined_entities  = merge_lists_with_priority(combined_entities,aux_entities)\n",
    "    # Convert the selected_entries dictionary back to a list\n",
    "    sent_dict[\"annotations\"] = combined_entities\n",
    "    if len(sent_dict[\"annotations\"]) > 0:\n",
    "        all_dict.append(sent_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Recurrent NSCLC with BRAF Class II alterations KRAS mutations other than TP53RK and G12C {ie, G12D, G12V} mutations {with Sponsor approval for KRAS mutations} without small cell lung cancer transformation with progressive disease confirmed by radiographic assessment.\".lower())\n",
    "text = \" \".join(doc._.bow)\n",
    "mutation_entities = mutations_pipeline(text, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_nlp = medspacy.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_nlp = medspacy.load()\n",
    "med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "med_nlp.disable_pipe('medspacy_pyrush')\n",
    "# med_nlp.add_pipe('sentencizer')\n",
    "print(med_nlp.pipe_names)\n",
    "@Language.component(\"gene-ner\")\n",
    "def gene_ner(doc):\n",
    "    spacy_entities = [(entity['entity_group'], entity['start'], entity['end']) for entity in entities_resolved]\n",
    "    for entity, start, end in spacy_entities:\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        # Find the corresponding tokens for the start and end positions\n",
    "        for token in doc:\n",
    "            if token.idx <= start < token.idx + len(token.text) and start_token is None:\n",
    "                start_token = token\n",
    "            if token.idx <= end <= token.idx + len(token.text) and end_token is None:\n",
    "                end_token = token\n",
    "\n",
    "        # Check if the start or end positions fall outside the tokenization\n",
    "        if start_token is None or end_token is None:\n",
    "            continue\n",
    "\n",
    "        span = spacy.tokens.Span(doc, start=start_token.i, end=end_token.i + 1, label=entity)\n",
    "        doc.ents = list(doc.ents) + [span]    \n",
    "    return doc\n",
    "\n",
    "\n",
    "med_nlp.add_pipe(\"gene-ner\", before='medspacy_context') \n",
    "\n",
    "@Language.component(\"biomed-ner\")\n",
    "def biomedical_ner(doc):\n",
    "    sp_entities = [(entity['entity_group'], entity['start'], entity['end']) for entity in entities_biomedical]\n",
    "    for entity, start, end in sp_entities:\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        # Find the corresponding tokens for the start and end positions\n",
    "        for token in doc:\n",
    "            if token.idx <= start < token.idx + len(token.text) and start_token is None:\n",
    "                start_token = token\n",
    "            if token.idx <= end <= token.idx + len(token.text) and end_token is None:\n",
    "                end_token = token\n",
    "\n",
    "        # Check if the start or end positions fall outside the tokenization\n",
    "        if start_token is None or end_token is None:\n",
    "            continue\n",
    "\n",
    "        span = spacy.tokens.Span(doc, start=start_token.i, end=end_token.i + 1, label=entity)\n",
    "        doc.ents = list(doc.ents) + [span]    \n",
    "    return doc\n",
    "\n",
    "# med_nlp.add_pipe(\"biomed-ner\", before='medspacy_context') \n",
    "\n",
    "@Language.component(\"aberrations-ner\")\n",
    "def regex_pattern_matcher_for_aberrations(doc):\n",
    "    df_regex = pd.read_csv(\"../data/regex_variants.tsv\", sep=\"\\t\", header=None)\n",
    "    df_regex = df_regex.rename(columns={1 : \"label\", 2:\"regex_pattern\"}).drop(columns=[0])\n",
    "    dict_regex = df_regex.set_index('label')['regex_pattern'].to_dict()\n",
    "    original_ents = list(doc.ents)\n",
    "    # Compile the regex patterns\n",
    "    compiled_patterns = {\n",
    "        label: re.compile(pattern)\n",
    "        for label, pattern in dict_regex.items()\n",
    "    }\n",
    "\n",
    "    mwt_ents = []\n",
    "    for label, pattern in compiled_patterns.items():\n",
    "        for match in re.finditer(pattern, doc.text):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end)\n",
    "            if span is not None:\n",
    "                mwt_ents.append((label, span.start, span.end, span.text))\n",
    "                \n",
    "    for ent in mwt_ents:\n",
    "        label, start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=label)\n",
    "        original_ents.append(per_ent)\n",
    "\n",
    "    doc.ents = filter_spans(original_ents)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "med_nlp.add_pipe(\"aberrations-ner\", before='medspacy_context') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list =[] \n",
    "for entity in doc.ents:\n",
    "    ent_list.append({\"entity_group\" : entity.label_, \"text\" : entity.text, \"start\": entity.start_char, \"end\": entity.end_char, \"is_negated\" : \"yes\" if entity._.is_negated else \"no\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aberration_recognizer(text):\n",
    "    med_nlp = medspacy.load()\n",
    "    med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "    @Language.component(\"aberrations-ner\")\n",
    "    def regex_pattern_matcher_for_aberrations(doc):\n",
    "        df_regex = pd.read_csv(\"../data/regex_variants.tsv\", sep=\"\\t\", header=None)\n",
    "        df_regex = df_regex.rename(columns={1 : \"label\", 2:\"regex_pattern\"}).drop(columns=[0])\n",
    "        dict_regex = df_regex.set_index('label')['regex_pattern'].to_dict()\n",
    "        original_ents = list(doc.ents)\n",
    "        # Compile the regex patterns\n",
    "        compiled_patterns = {\n",
    "            label: re.compile(pattern)\n",
    "            for label, pattern in dict_regex.items()\n",
    "        }\n",
    "        mwt_ents = []\n",
    "        for label, pattern in compiled_patterns.items():\n",
    "            for match in re.finditer(pattern, doc.text):\n",
    "                start, end = match.span()\n",
    "                span = doc.char_span(start, end)\n",
    "                if span is not None:\n",
    "                    mwt_ents.append((label, span.start, span.end, span.text))\n",
    "                    \n",
    "        for ent in mwt_ents:\n",
    "            label, start, end, name = ent\n",
    "            per_ent = Span(doc, start, end, label=label)\n",
    "            original_ents.append(per_ent)\n",
    "\n",
    "        doc.ents = filter_spans(original_ents)\n",
    "        \n",
    "        return doc\n",
    "    med_nlp.add_pipe(\"aberrations-ner\", before='medspacy_context')\n",
    "    doc = med_nlp(text)\n",
    "    return doc\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = aberration_recognizer(\"Cohort 1: Recurrent NSCLC with BRAF Class II alterations or KRAS mutations other than G12C {ie, G12D, G12V} mutations {with Sponsor approval for KRAS mutations} without small cell lung cancer transformation with progressive disease confirmed by radiographic assessment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to modify options for displacy NER visualization\n",
    "def get_entity_options():\n",
    "    entities = [\n",
    "        \"Disease_disorder\", \"CHEMICAL\", \"Age\", \"GENETIC\", \"Duration\", \"Date\", \"Sex\",\n",
    "        \"Diagnostic_procedure\", \"Lab_value\", \"Protein\", \"DNA\", \"cell_type\",\n",
    "        \"Sign_symptom\", \"expression\", \"mutation\", \"NEG_ENTITY\",\n",
    "    ]\n",
    "    colors = {\n",
    "        \"Disease_disorder\": \"linear-gradient(180deg, #66ffcc, #abf763)\",\n",
    "        \"CHEMICAL\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "        \"Age\": \"linear-gradient(180deg, #ff9a8f, #ffb55e)\",\n",
    "        \"GENETIC\": \"linear-gradient(90deg, #9cd1fc, #9cfcf6)\",\n",
    "        \"Duration\": \"linear-gradient(180deg, #fe8ce6, #fe8cd9)\",\n",
    "        \"Date\": \"linear-gradient(90deg, #fca79c, #fcc59c)\",\n",
    "        \"Sex\": \"linear-gradient(180deg, #9cfdfe, #9c9dfc)\",\n",
    "        \"Diagnostic_procedure\": \"linear-gradient(90deg, #fcb69c, #fcec9c)\",\n",
    "        \"Lab_value\": \"linear-gradient(180deg, #9cfc9c, #e3fc9c)\",\n",
    "        \"Protein\": \"linear-gradient(90deg, #fc9cb0, #fc9cbe)\",\n",
    "        \"DNA\": \"linear-gradient(180deg, #9c9cfc, #a39cfc)\",\n",
    "        \"cell_type\": \"linear-gradient(90deg, #9ccdfc, #9cc3fc)\",\n",
    "        \"Sign_symptom\": \"linear-gradient(180deg, #9cfc9e, #d4fc9c)\",\n",
    "        \"expression\": \"linear-gradient(90deg, #9cfc9e, #fc9c9c)\",\n",
    "        \"mutation\": \"linear-gradient(180deg, #ffc766, #fc9c9c)\",\n",
    "        \"NEG_ENTITY\": \"linear-gradient(180deg, #ffc766, #fc9c9c)\"\n",
    "    }\n",
    "    options = {\"ents\": entities, \"colors\": colors}\n",
    "    return options\n",
    "\n",
    "options = get_entity_options()#visualizing identified Named Entities in clinical input text \n",
    "displacy.render(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_handling(doc):\n",
    "    results = []\n",
    "    for e in doc.ents:\n",
    "        rs = str(e._.is_negated)\n",
    "        if rs == \"True\": \n",
    "            results.append(e.text)\n",
    "    return results #list of negative concepts from clinical note identified by negspacy\n",
    "\n",
    "results0 = negation_handling(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to identify span objects of matched megative phrases from clinical note\n",
    "def match(nlp,terms,label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        matcher = PhraseMatcher(nlp.vocab)\n",
    "        matcher.add(label, None, *patterns)\n",
    "        return matcher\n",
    "    \n",
    "#replacing the labels for identified negative entities    \n",
    "def overwrite_ent_lbl(matcher, doc):\n",
    "    matches = matcher(doc)\n",
    "    seen_tokens = set()\n",
    "    new_entities = []\n",
    "    entities = doc.ents\n",
    "    for match_id, start, end in matches:\n",
    "        if start not in seen_tokens and end - 1 not in seen_tokens:\n",
    "            new_entities.append(Span(doc, start, end, label=match_id))\n",
    "            entities = [\n",
    "                e for e in entities if not (e.start < end and e.end > start)\n",
    "            ]\n",
    "            seen_tokens.update(range(start, end))\n",
    "        doc.ents = tuple(entities) + tuple(new_entities)\n",
    "    return doc\n",
    "\n",
    "matcher = match(med_nlp, results0, \"NEG_ENTITY\")\n",
    "#doc0: new doc object with added \"NEG_ENTITY label\"\n",
    "doc0 = overwrite_ent_lbl(matcher, doc) #visualizing identified Named Entities in clinical input text \n",
    "displacy.render(doc0, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add custom negation terms to the existing model\n",
    "from negspacy.termsets import termset\n",
    "ts = termset(\"en_clinical\")\n",
    "ts.add_patterns({\n",
    "            \"preceding_negations\": [\"deny\", \"refuse\", \"neither\", \"nor\", \"do not have\"],\n",
    "            \"following_negations\": [\"absence of\", \"deny\", \"decline\"],\n",
    "        })\n",
    "def neg_model2(nlp_model):\n",
    "    nlp = spacy.load(nlp_model, disable = ['parser'])\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    nlp.add_pipe(\"negex\")\n",
    "    return nlp  #updated list of all the negative concepts from clinical note identified by negspacy\n",
    "results1 = negation_handling(\"en_ner_bc5cdr_md\", lem_clinical_note, neg_model2)\n",
    "matcher = match(nlp1, results1, \"NEG_ENTITY\")\n",
    "#doc1: new doc object with added custom concepts for \"NEG_ENTITY label\"\n",
    "doc1 = overwrite_ent_lbl(matcher, doc) #visualizing identified Named Entities in clinical input text \n",
    "displacy.render(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "import pandas as pd\n",
    "from utils import tokenize\n",
    "\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "# Text to search for matches\n",
    "text = \"A Randomized Phase II Trial of a Mutated gp100 Melanoma Peptide g209-217210M With Hight Dose Interleukin-2 IL-2 in HLA-A2.1+Patients With Metastatic Melanoma\"\n",
    "\n",
    "@Language.component(\"aberrations-ner\")\n",
    "def regex_pattern_matcher_for_aberrations(doc):\n",
    "    df_regex = pd.read_csv(\"../data/regex_variants.tsv\", sep=\"\\t\", header=None)\n",
    "    df_regex = df_regex.rename(columns={1 : \"label\", 2:\"regex_pattern\"}).drop(columns=[0])\n",
    "    dict_regex = df_regex.set_index('label')['regex_pattern'].to_dict()\n",
    "    original_ents = list(doc.ents)\n",
    "    # Compile the regex patterns\n",
    "    compiled_patterns = {\n",
    "        label: re.compile(pattern)\n",
    "        for label, pattern in dict_regex.items()\n",
    "    }\n",
    "\n",
    "    mwt_ents = []\n",
    "    for label, pattern in compiled_patterns.items():\n",
    "        for match in re.finditer(pattern, doc.text):\n",
    "            start, end = match.span()\n",
    "            span = doc.char_span(start, end)\n",
    "            if span is not None:\n",
    "                mwt_ents.append((label, span.start, span.end, span.text))\n",
    "                \n",
    "    for ent in mwt_ents:\n",
    "        label, start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=label)\n",
    "        original_ents.append(per_ent)\n",
    "\n",
    "    doc.ents = filter_spans(original_ents)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"aberrations-ner\", last=True) \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Replace 'input_file.bin' and 'output_file.txt' with your file names\n",
    "\n",
    "# Read binary data from the .bin file\n",
    "with open('wikipedia-pubmed-and-PMC-w2v.bin', 'rb') as binary_file:\n",
    "    binary_data = binary_file.read()\n",
    "\n",
    "# Encode binary data to Base64\n",
    "base64_encoded_data = base64.b64encode(binary_data)\n",
    "\n",
    "# Convert bytes to a string and write to a .txt file\n",
    "with open('output_vectors.txt', 'w') as text_file:\n",
    "    text_file.write(base64_encoded_data.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the byte position from where you want to start printing\n",
    "# Number of bytes you want to print\n",
    "num_bytes_to_print = 100\n",
    "\n",
    "# Read binary data from the .bin file\n",
    "with open('wikipedia-pubmed-and-PMC-w2v.bin', 'rb') as binary_file:\n",
    "    binary_data = binary_file.read()\n",
    "\n",
    "# Slice the binary data to get only the first N bytes\n",
    "sliced_data = binary_data[6000000:6000100]\n",
    "print(sliced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Replace 'path_to_word2vec_model.bin' and 'output_file.txt' with appropriate values\n",
    "model = KeyedVectors.load_word2vec_format('wikipedia-pubmed-and-PMC-w2v.bin', binary=True)\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open('output_file.txt', 'w') as output_file:\n",
    "    # Iterate over each word in the vocabulary\n",
    "    for idx, word in enumerate(model.index_to_key):\n",
    "        print(idx)\n",
    "        # Get the word vector for the current word\n",
    "        word_vector = model.get_vector(word)\n",
    "\n",
    "        # Convert the word vector to a comma-separated string\n",
    "        vector_str = ','.join(str(val) for val in word_vector)\n",
    "\n",
    "        # Write the word and its vector to the file\n",
    "        output_file.write(f\"{word} {vector_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "# Replace 'path_to_word2vec_model.bin' and 'spacy_word2vec_model' with appropriate values\n",
    "gensim_model = KeyedVectors.load_word2vec_format('wikipedia-pubmed-and-PMC-w2v.bin', binary=True)\n",
    "\n",
    "# Create a new spaCy Language object with a blank Vocab\n",
    "nlp = spacy.blank(\"en\")\n",
    "vocab = Vocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy word vectors from Gensim model to spaCy Vocab\n",
    "for word in gensim_model.index_to_key:\n",
    "    print(word)\n",
    "    vocab.set_vector(word, gensim_model[word])\n",
    "\n",
    "# Set the spaCy Vocab for the spaCy Language object\n",
    "nlp.vocab = vocab\n",
    "\n",
    "# # Save the spaCy model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"spacy_word2vec_biomed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "MESH_RDF_URL = \"https://id.nlm.nih.gov/mesh/sparql\"\n",
    "MESH_GRAPH = rdflib.Graph()\n",
    "\n",
    "def fetch_mesh_data():\n",
    "    MESH_GRAPH.parse(MESH_RDF_URL, format=\"xml\")\n",
    "\n",
    "# Fetch MeSH data\n",
    "fetch_mesh_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "py_text = \"Cohort 1: Recurrent NSCLC with BRAF Class II alterations or KRAS mutations other than G12C {ie, G12D, G12V} mutations {with Sponsor approval for KRAS mutations} without small cell lung cancer transformation with progressive disease confirmed by radiographic assessment.\"\n",
    "py_nlp = spacy.load(\"en_core_web_sm\")\n",
    "py_doc = py_nlp(py_text)\n",
    "displacy.render(py_doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "target_directory = \"/home/mabdallah/BERN2/multi_ner/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "# Specify the path to your .sh script and the directory you want to change to\n",
    "current_directory = os.getcwd()\n",
    "run_path = \"/home/mabdallah/BERN2/scripts/run_bern2.sh\"\n",
    "stop_path = \"/home/mabdallah/BERN2/scripts/stop_bern2.sh\"\n",
    "working_directory = \"/home/mabdallah/BERN2/scripts/\"\n",
    "os.chdir(working_directory)\n",
    "print(\"Stopping existing Bio-NER server instance.\")\n",
    "stop_process = subprocess.Popen([\"bash\", stop_path], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "stop_process.wait()\n",
    "print(\"Activating Bio-NER Server. This can take between 30 seconds and 1 minute.\")\n",
    "try:\n",
    "    subprocess.Popen([\"bash\", run_path], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "    timeout = 60  # Adjust this value as needed\n",
    "    # Define the server's URL that you want to check\n",
    "    server_url = \"http://localhost:8888\"  # Update with the actual URL\n",
    "    # Wait for the server to become available or reach the timeout\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            # Send a request to the server to check its availability\n",
    "            response = requests.get(server_url)\n",
    "            response.raise_for_status()  # Raises an exception for non-2xx status codes\n",
    "            break  # Server is available, exit the loop\n",
    "        except (requests.ConnectionError, requests.HTTPError) as e:\n",
    "            if time.time() - start_time >= timeout:\n",
    "                print(f\"Server did not become available within {timeout} seconds.\")\n",
    "                break  # Timeout reached\n",
    "            else:\n",
    "                # Wait for a short time before checking again\n",
    "                time.sleep(1)\n",
    "\n",
    "    # Continue with other tasks\n",
    "    print(\"Server is now available.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error executing the script: {e}\")\n",
    "os.chdir(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_overlap(tagged_docs, tmvar_docs):\n",
    "        \"\"\"\n",
    "        Step 1: check CUI and logit probability for same mention\n",
    "        Step 2: check overlap with mutation and tags with the highest probability\n",
    "        \"\"\"\n",
    "\n",
    "        # [Step 1] compare CUI and probability for same mention\n",
    "        span2mentions = {}\n",
    "        for entity_type, entity_dict in tagged_docs[0]['entities'].items():\n",
    "            # check CUI and probability\n",
    "            for mention_idx, mention_dict in enumerate(entity_dict):\n",
    "                start = mention_dict['start']\n",
    "                end = mention_dict['end']\n",
    "                if \"%d-%d\" % (start, end) not in span2mentions:\n",
    "                    span2mentions[\"%d-%d\" % (start, end)] = []\n",
    "                \n",
    "                span2mentions[\"%d-%d\"%(start, end)].append({\"type\":entity_type,\n",
    "                                                            \"CUI\": mention_dict['id'],\n",
    "                                                            \"check_CUI\": 1 if mention_dict['id'] != 'CUI-less' else 0,\n",
    "                                                            \"prob\": tagged_docs[0]['prob'][entity_type][mention_idx][1],\n",
    "                                                            \"is_neural_normalized\":mention_dict['is_neural_normalized']})\n",
    "        \n",
    "        for span in span2mentions.keys():\n",
    "            # sort elements with CUI\n",
    "            span2mentions[span] = sorted(span2mentions[span], key=lambda x:(x['check_CUI'], x['prob']), reverse=True)\n",
    "\n",
    "        for entity_type, entity_dict in tagged_docs[0]['entities'].items():\n",
    "            update_list = []\n",
    "            for mention_idx, mention_dict in enumerate(entity_dict):\n",
    "                start = mention_dict['start']\n",
    "                end = mention_dict['end']\n",
    "                \n",
    "                if span2mentions[\"%d-%d\"%(start, end)][0]['CUI'] == mention_dict['id'] and span2mentions[\"%d-%d\"%(start, end)][0]['type'] == entity_type:\n",
    "                    update_list.append(mention_dict)\n",
    "\n",
    "            tagged_docs[0]['entities'].update({entity_type:update_list})\n",
    "\n",
    "        # [Step 2] add mutation annotation\n",
    "        tagged_docs[0]['entities']['mutation'] = tmvar_docs[0]['entities']['mutation']\n",
    "        print(tmvar_docs)\n",
    "        return tagged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from downloader import Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Downloader(id_list=[\"NCT05786924\"], n_jobs=5).download_and_update_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootfile = \"../data/preprocessed_data/\"\n",
    "rootfile + \"/pre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Preprocessor(id_list=[\"NCT05786924\"], n_jobs=5).preprocess_clinical_trials_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_remove_overlaps(dictionary_list, preferred_groups):\n",
    "    # Create a dictionary to store non-overlapping entries\n",
    "    non_overlapping = {}\n",
    "\n",
    "    # Create a set from the preferred_groups for faster membership checking\n",
    "    preferred_set = set(preferred_groups)\n",
    "\n",
    "    # Iterate through the input list\n",
    "    for entry in dictionary_list:\n",
    "        text = entry['text']\n",
    "        group = entry['entity_group']\n",
    "\n",
    "        # Check if the text is already in the non_overlapping dictionary\n",
    "        if text in non_overlapping:\n",
    "            # Compare groups and keep the entry if it belongs to one of the preferred groups\n",
    "            if group in preferred_set:\n",
    "                non_overlapping[text] = entry\n",
    "        else:\n",
    "            non_overlapping[text] = entry\n",
    "\n",
    "    # Convert the non-overlapping dictionary back to a list\n",
    "    result_list = list(non_overlapping.values())\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_and_remove_overlaps(sentence[\"annotations\"], preferred_groups=[\"gene\", \"ProteinMutation\", \"DNAMutation\", \"SNP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_remove_overlaps(dictionary_list, preferred_groups):\n",
    "    # Create a dictionary to store non-overlapping entries\n",
    "    non_overlapping = {}\n",
    "\n",
    "    # Create a set from the preferred_groups for faster membership checking\n",
    "    preferred_set = set(preferred_groups)\n",
    "\n",
    "    # Iterate through the input list\n",
    "    for entry in dictionary_list:\n",
    "        text = entry['text']\n",
    "        group = entry['group']\n",
    "\n",
    "        # Check if the text is already in the non_overlapping dictionary\n",
    "        if text in non_overlapping:\n",
    "            # Compare groups and keep the entry if it belongs to one of the preferred groups\n",
    "            if group in preferred_set:\n",
    "                non_overlapping[text] = entry\n",
    "        else:\n",
    "            non_overlapping[text] = entry\n",
    "\n",
    "    # Convert the non-overlapping dictionary back to a list\n",
    "    result_list = list(non_overlapping.values())\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# Example usage:\n",
    "input_list = [\n",
    "    {'text': 'apple', 'group': 'A'},\n",
    "    {'text': 'banana', 'group': 'B'},\n",
    "    {'text': 'apple', 'group': 'B'},\n",
    "    {'text': 'banana', 'group': 'C'},\n",
    "    {'text': 'date', 'group': 'C'},\n",
    "]\n",
    "\n",
    "preferred_groups = ['A', 'B']\n",
    "\n",
    "result = find_and_remove_overlaps(input_list, preferred_groups)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_handling(sentence, entity):\n",
    "    med_nlp = medspacy.load()\n",
    "    med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "    # med_nlp.disable_pipe('medspacy_pyrush')\n",
    "    @Language.component(\"add_custom_entity\")\n",
    "    def add_cutom_entity(doc):\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        # Find the corresponding tokens for the start and end positions\n",
    "        for token in doc:\n",
    "            if token.idx <= entity[\"start\"] < token.idx + len(token.text) and start_token is None:\n",
    "                start_token = token\n",
    "            if token.idx <= entity[\"end\"] <= token.idx + len(token.text) and end_token is None:\n",
    "                end_token = token\n",
    "        doc.set_ents([Span(doc, start_token.i, end_token.i + 1, entity[\"entity_group\"])]) \n",
    "        return doc\n",
    "    med_nlp.add_pipe(\"add_custom_entity\", before='medspacy_context') \n",
    "    doc = med_nlp(sentence)\n",
    "    for e in doc.ents:\n",
    "        rs = str(e._.is_negated)\n",
    "        if rs == \"True\": \n",
    "            entity[\"is_negated\"] = \"yes\"\n",
    "        else:\n",
    "            entity[\"is_negated\"] = \"no\"\n",
    "    return  entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict = ent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict[[\"entity_group\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list[0][\"sentence\"] = \"No Histologically or cytologically confirmed recurrent/advanced metastatic solid tumors nor histiocytic neoplasms with documented BRAF or RAS {NRAS or KRAS} mutations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_list[0][\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, World! (This) is an example sentence.\"\n",
    "\n",
    "# Remove punctuation using regex\n",
    "clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mabdallah/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from entity_recognition import EntityRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco = EntityRecognizer(n_jobs=1, id_list=[\"NCT05127174\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'is_negated'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/mabdallah/NLPtrials/src/inference.ipynb Cell 64\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbaracuda/home/mabdallah/NLPtrials/src/inference.ipynb#Y120sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m entities \u001b[39m=\u001b[39m reco()\n",
      "File \u001b[0;32m/scratch/homedirs/mabdallah/NLPtrials/src/entity_recognition.py:258\u001b[0m, in \u001b[0;36mEntityRecognizer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_loader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_list)\n\u001b[1;32m    257\u001b[0m parallel_runner \u001b[39m=\u001b[39m ParallelExecutor(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid_list))\n\u001b[0;32m--> 258\u001b[0m X \u001b[39m=\u001b[39m parallel_runner(\n\u001b[1;32m    259\u001b[0m     joblib\u001b[39m.\u001b[39;49mdelayed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecognize_entities)(\n\u001b[1;32m    260\u001b[0m     ct_df, \n\u001b[1;32m    261\u001b[0m     )\n\u001b[1;32m    262\u001b[0m     \u001b[39mfor\u001b[39;49;00m ct_df \u001b[39min\u001b[39;49;00m df\n\u001b[1;32m    263\u001b[0m )     \n\u001b[1;32m    264\u001b[0m all_trials \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(X)\n\u001b[1;32m    265\u001b[0m all_trials\u001b[39m.\u001b[39mto_csv(OUTPUT_FILEPATH_CT \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mentities_parsed.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/homedirs/mabdallah/NLPtrials/src/entity_recognition.py:55\u001b[0m, in \u001b[0;36mParallelExecutor.<locals>.aprun.<locals>.tmp\u001b[0;34m(op_iter)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValue \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m not supported as bar type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m bar)\n\u001b[1;32m     54\u001b[0m \u001b[39m# Pass n_jobs from joblib_args\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[39mreturn\u001b[39;00m joblib\u001b[39m.\u001b[39;49mParallel(n_jobs\u001b[39m=\u001b[39;49mjoblib_args\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mn_jobs\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m10\u001b[39;49m))(bar_func(op_iter))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/scratch/homedirs/mabdallah/NLPtrials/src/entity_recognition.py:233\u001b[0m, in \u001b[0;36mEntityRecognizer.recognize_entities\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m ent \u001b[39mand\u001b[39;00m ent[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m) \u001b[39mor\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ent):\n\u001b[1;32m    232\u001b[0m     ent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnegation_handling(sent, ent)\n\u001b[0;32m--> 233\u001b[0m     is_negated\u001b[39m.\u001b[39mappend(ent[\u001b[39m\"\u001b[39;49m\u001b[39mis_negated\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \n\u001b[1;32m    234\u001b[0m     nct_ids\u001b[39m.\u001b[39mappend(row[\u001b[39m\"\u001b[39m\u001b[39mnct_id\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    235\u001b[0m     sentences\u001b[39m.\u001b[39mappend(sent)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'is_negated'"
     ]
    }
   ],
   "source": [
    "entities = reco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\", device=0) # pass device=0 if using gpu\n",
    "pipe(\"\"\"has not undergone a hysterectomy or bilateral oophorectomy\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "from spacy.tokens import Span\n",
    "entity = {'entity_group': 'Therapeutic_procedure', 'score': 0.99969435, 'text': 'hysterectomy', 'start': 20, 'end': 32}\n",
    "sentence = \"has not undergone a hysterectomy or bilateral oophorectomy, or\"\n",
    "\n",
    "def negation_handling(sentence, entity):\n",
    "\n",
    "    med_nlp = medspacy.load()\n",
    "    med_nlp.disable_pipe('medspacy_target_matcher')\n",
    "    @Language.component(\"add_custom_entity\")\n",
    "    def add_cutom_entity(doc):\n",
    "\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        # Find the corresponding tokens for the start and end positions\n",
    "        for token in doc:\n",
    "            if token.idx <= entity[\"start\"] < token.idx + len(token.text) and start_token is None:\n",
    "                start_token = token\n",
    "            if token.idx <= entity[\"end\"] <= token.idx + len(token.text) and end_token is None:\n",
    "                end_token = token\n",
    "        doc.set_ents([Span(doc, start_token.i, end_token.i + 1, entity[\"entity_group\"])]) \n",
    "        return doc\n",
    "    med_nlp.add_pipe(\"add_custom_entity\", before='medspacy_pyrush') \n",
    "    doc = med_nlp(sentence)\n",
    "    for e in doc.ents:\n",
    "        rs = str(e._.is_negated)\n",
    "        if rs == \"True\": \n",
    "            entity[\"is_negated\"] = \"yes\"\n",
    "        else:\n",
    "            entity[\"is_negated\"] = \"no\"\n",
    "    return  entity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_handling(sentence=sentence, entity=entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity[\"start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})\n",
    "nlp = medspacy.load(nlp)\n",
    "nlp.disable_pipe('medspacy_target_matcher')\n",
    "nlp.disable_pipe('medspacy_pyrush')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Platelets > 50 x 109/L with no platelet transfusions in the prior 7 days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is an example sentence with some text in it.\"\n",
    "substring = \"example sentence\"\n",
    "\n",
    "# Find the start and end positions of the substring\n",
    "start = sentence.find(substring)\n",
    "end = start + len(substring) - 1  # Subtract 1 to get the inclusive end position\n",
    "\n",
    "if start != -1:\n",
    "    print(f\"Start position: {start}\")\n",
    "    print(f\"End position: {end}\")\n",
    "else:\n",
    "    print(\"Substring not found in the sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
